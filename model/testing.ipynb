{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e2f1536-f4e8-4865-8723-a1a0f6567bb9",
   "metadata": {},
   "source": [
    "# Testing words\n",
    "This notebook provides some skeleton code for loading the training data and getting the predictions from the model for the different keywords.\n",
    "\n",
    "## Warning: For now, I am still training the models, testing can already be done with the agency model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc76a0f5-a0b5-4402-96e1-27dbf019245e",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b154045-3ec8-4845-9133-1aa8e20611a0",
   "metadata": {},
   "source": [
    "This section of the notebook makes sure you have all the libraries installed that are used by the code.\n",
    "It also makes sure that they are updated to the newest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e12dfcf9-55da-4dda-b311-107dd44546a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers -Uqq\n",
    "%pip install nltk -Uqq\n",
    "%pip install matplotlib -Uqq\n",
    "%pip install wordcloud -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156f0d90-3a40-407e-9c8f-696b28b078fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    TextClassificationPipeline,\n",
    ")\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe45fc5-4066-44cc-92ea-5dd65c9a51b4",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f753a86b-2b4e-4b37-be19-2a997130693b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SoftBank-owned Arm to launch new AI chip for small devices',\n",
       " \"What's your favorite scary movie? AI gets a 'ghoulish assignment' to reimagine classic horror film posters - from movies such as Scream and Child's Play - and the results that are BLOODIER and more terrifying than the originals\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "dataset = []\n",
    "\n",
    "with open(\"data/data.json\") as file:\n",
    "    dataset = list(map(lambda x: x[\"text\"], json.load(file)[\"data\"]))\n",
    "\n",
    "random.shuffle(dataset)\n",
    "dataset[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d89d4-a773-46ba-916f-68109ca28ae3",
   "metadata": {},
   "source": [
    "## Loading model for some keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecd763b4-0c49-454a-9fa5-8df1ebaf7782",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"agency\",\n",
    "    # \"suggestiveImagery\",\n",
    "    \"comparisonWithHumanIntelligence\",\n",
    "    \"comparisonWithHumanSkills\",\n",
    "    \"hyperbole\",\n",
    "    \"uncriticalHistoryComparison\",\n",
    "    \"unjustifiedClaimsAboutFuture\",\n",
    "    \"falseClaimsAboutProgress\",\n",
    "    \"incorrectClaimsAboutStudyReport\",\n",
    "    \"deepSoundingTermsForBanalities\",\n",
    "    \"treatingSpokespeopleAsNeutral\",\n",
    "    \"repeatingPRTerms\",\n",
    "    \"noDiscussionOfLimitations\",\n",
    "    \"deEmphasizingLimitations\",\n",
    "    \"limitationsAddressedBySkeptics\",\n",
    "    \"downplayingHumanLabour\",\n",
    "    \"performanceNumbersWithoutCaveats\",\n",
    "    # \"inscrutability\",\n",
    "]\n",
    "\n",
    "models = {}\n",
    "\n",
    "for label in labels:\n",
    "    models[label] = BertForSequenceClassification.from_pretrained(\n",
    "        f\"xt0r3/aihype_{label}-vs-rest\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf84476-8157-4603-aeab-30b3659f7687",
   "metadata": {},
   "source": [
    "## Adding input processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc79e0ce-8250-4ee0-a85b-4d0da5c7744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8b01703-effa-461b-9383-c4f76145e884",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipes = {\n",
    "    label: TextClassificationPipeline(\n",
    "        model=models[label],\n",
    "        tokenizer=tokenizer,\n",
    "        top_k=None,\n",
    "    )\n",
    "    for label in labels\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8010627c-6682-48bc-a6d0-ebadcfb25773",
   "metadata": {},
   "source": [
    "## Define prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e5fa856-040a-4064-9660-0147133d00ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(preds):\n",
    "    for pred in preds:\n",
    "        if pred[\"label\"] == \"LABEL_1\":\n",
    "            return pred[\"score\"] >= 0.5\n",
    "    return False\n",
    "\n",
    "\n",
    "def predict(text, label):\n",
    "    preds = pipes[label](text)[0]\n",
    "    return get_result(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42247f4c-7767-459b-bb79-dcbbccf6c198",
   "metadata": {},
   "source": [
    "## Playing around with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83a36ce0-cd78-4733-ba45-6568edc91ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SoftBank-owned Arm to launch new AI chip for small devices',\n",
       " \"What's your favorite scary movie? AI gets a 'ghoulish assignment' to reimagine classic horror film posters - from movies such as Scream and Child's Play - and the results that are BLOODIER and more terrifying than the originals\",\n",
       " 'French tax officials use AI to spot 20,000 undeclared pools',\n",
       " 'Vic-made robot to fight African poachers']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e518ec8c-a7cc-4d95-bb8f-84052be74b44",
   "metadata": {},
   "source": [
    "## Tokenization example \n",
    "This chapter shows an example of tokenization so that you can do data classification easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb36ad01-f6ce-4fa2-80b7-63a8b2489a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SoftBank-owned',\n",
       " 'Arm',\n",
       " 'to',\n",
       " 'launch',\n",
       " 'new',\n",
       " 'AI',\n",
       " 'chip',\n",
       " 'for',\n",
       " 'small',\n",
       " 'devices']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK tokenizer for human-understood words\n",
    "word_tokenize(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cd0f6a7-d8ab-46bc-a422-c4e57af3a51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['infinitesimal']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"infinitesimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da77b87f-0b89-465a-a9ac-fa3941d9072c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Soft',\n",
       " '##B',\n",
       " '##an',\n",
       " '##k',\n",
       " '-',\n",
       " 'owned',\n",
       " 'Arm',\n",
       " 'to',\n",
       " 'launch',\n",
       " 'new',\n",
       " 'AI',\n",
       " 'chip',\n",
       " 'for',\n",
       " 'small',\n",
       " 'devices']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT tokeznier for subwords the model pays attention to.\n",
    "# This does not only find words, but also splits some long words to smaller subwords.\n",
    "tokenizer.tokenize(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8df07d56-da44-4b3e-bb80-357cf67b9356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['infinite', '##si', '##mal']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"infinitesimal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c594f-bc73-40f1-b0ce-7f6610a09a69",
   "metadata": {},
   "source": [
    "## Getting the word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7640bb22-ec3d-4aba-8537-f03dbeaf8d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_nltk = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "freq_bert = defaultdict(lambda: defaultdict(lambda: 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d505d789-d529-454a-abe2-48b9f2e4c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_headline(headline, label):\n",
    "    if predict(headline, label):\n",
    "        for word in tokenize_nltk:\n",
    "            freq_nltk[label][word] += 1\n",
    "        for word in tokenize_bert:\n",
    "            freq_bert[label][word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "747cafbd-163d-4fa2-a74a-02e49cdab77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0/238\n",
      "Progress: 20/238\n",
      "Progress: 40/238\n",
      "Progress: 60/238\n",
      "Progress: 80/238\n",
      "Progress: 100/238\n",
      "Progress: 120/238\n",
      "Progress: 140/238\n",
      "Progress: 160/238\n",
      "Progress: 180/238\n",
      "Progress: 200/238\n",
      "Progress: 220/238\n"
     ]
    }
   ],
   "source": [
    "for i, headline in enumerate(dataset):\n",
    "    tokenize_nltk = word_tokenize(headline)\n",
    "    tokenize_bert = tokenizer.tokenize(headline)\n",
    "\n",
    "    pool = ThreadPool(4)\n",
    "    \n",
    "    func = partial(process_headline, headline)\n",
    "    pool.map(func, labels)\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Progress: {i}/{len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a20062-a377-456d-96dd-c9a13a71fc0a",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "644ea603-a39f-4adc-8090-cf35abbadd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai', 'ml', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "STOPWORDS = stopwords.words(\"english\")\n",
    "\n",
    "with open('data/keywords') as file:\n",
    "    lines = [line.strip().lower() for line in file.readlines()]\n",
    "    print(lines[0:3])\n",
    "    STOPWORDS.extend(lines)\n",
    "    \n",
    "for label in labels:\n",
    "    freq_nltk[label] = {\n",
    "        k: v\n",
    "        for k, v in freq_nltk[label].items()\n",
    "        if k.lower() not in STOPWORDS\n",
    "    }\n",
    "    freq_bert[label] = {\n",
    "        k: v\n",
    "        for k, v in freq_bert[label].items()\n",
    "        if k.lower() not in STOPWORDS\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079ecde2-6bc3-4d97-b82a-5209d47e43bd",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee5f20c0-9251-4aef-8a0f-5df123c78e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13757/1933474960.py:1: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats(\"svg\")\n"
     ]
    }
   ],
   "source": [
    "set_matplotlib_formats(\"svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fbfccbe6-481c-401a-bb12-17c2f41f6ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot word cloud\n",
    "def plot_cloud(axs, wordcloud, title, i):\n",
    "    x = i // 4\n",
    "    y = i % 4\n",
    "    # Set figure size\n",
    "    # Display image\n",
    "    axs[x, y].imshow(wordcloud)\n",
    "    # No axis details\n",
    "    axs[x, y].axis(\"off\")\n",
    "    axs[x, y].set_title(title, fontdict={\"fontsize\": 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587da53c-a9b0-454f-bc1f-d5e2832b7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plot characteristics\n",
    "plt.figure(figsize=(44, 36))\n",
    "fig, axs = plt.subplots(4, 4, figsize=(10, 8))\n",
    "plt.rcParams[\"figure.dpi\"] = 600\n",
    "plt.rcParams[\"savefig.dpi\"] = 600\n",
    "\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    if len(freq_nltk[label]) == 0:\n",
    "        continue\n",
    "    # Generate wordcloud for NLTK text\n",
    "    wordcloud = WordCloud(\n",
    "        width=600,\n",
    "        height=400,\n",
    "        random_state=1,\n",
    "        background_color=\"black\",\n",
    "        colormap=\"Set2\",\n",
    "        collocations=False,\n",
    "    ).generate_from_frequencies(freq_nltk[label])\n",
    "    # Plot\n",
    "    plot_cloud(axs, wordcloud, label, i)\n",
    "\n",
    "plt.savefig(\"nltk.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd411d09-8192-4828-85b5-130c1c30aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plot characteristics\n",
    "plt.figure(figsize=(44, 36))\n",
    "fig, axs = plt.subplots(4, 4, figsize=(10, 8))\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "plt.rcParams[\"savefig.dpi\"] = 300\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    if len(freq_nltk[label]) == 0:\n",
    "        continue\n",
    "    # Generate wordcloud for BERT text\n",
    "    wordcloud = WordCloud(\n",
    "        width=600,\n",
    "        height=400,\n",
    "        random_state=1,\n",
    "        background_color=\"black\",\n",
    "        colormap=\"Set2\",\n",
    "        collocations=False,\n",
    "    ).generate_from_frequencies(freq_bert[label])\n",
    "    # Plot\n",
    "    plot_cloud(axs, wordcloud, label, i)\n",
    "\n",
    "plt.savefig(\"bert.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7611e6d2-6b2c-421c-898d-a01d717005c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(freq_nltk[labels[0]].items())[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe3c0a-44d0-4093-a4cb-f372a8f4e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(freq_nltk[labels[7]].items())[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6bd34-bbee-4d1b-aa0e-8daa80ee6c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
