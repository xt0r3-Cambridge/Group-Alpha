{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e2f1536-f4e8-4865-8723-a1a0f6567bb9",
   "metadata": {},
   "source": [
    "# Testing words\n",
    "This notebook provides some skeleton code for loading the training data and getting the predictions from the model for the different keywords.\n",
    "\n",
    "## Warning: For now, I am still training the models, testing can already be done with the agency model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc76a0f5-a0b5-4402-96e1-27dbf019245e",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b154045-3ec8-4845-9133-1aa8e20611a0",
   "metadata": {},
   "source": [
    "This section of the notebook makes sure you have all the libraries installed that are used by the code.\n",
    "It also makes sure that they are updated to the newest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e12dfcf9-55da-4dda-b311-107dd44546a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers -Uqq\n",
    "%pip install nltk -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "156f0d90-3a40-407e-9c8f-696b28b078fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    TextClassificationPipeline,\n",
    "    BertTokenizer,\n",
    ")\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe45fc5-4066-44cc-92ea-5dd65c9a51b4",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f753a86b-2b4e-4b37-be19-2a997130693b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A new vision of artificial intelligence for the people',\n",
       " 'The gig workers fighting back against the algorithms']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "with open('data/data.json') as file:\n",
    "    dataset = list(map(lambda x: x['text'], json.load(file)['data']))\n",
    "    \n",
    "dataset[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d89d4-a773-46ba-916f-68109ca28ae3",
   "metadata": {},
   "source": [
    "## Loading model for some keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecd763b4-0c49-454a-9fa5-8df1ebaf7782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79fc2d4aa6e47429decdf73826bca37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/745 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f58dbca086f4ddd9304167430ce9d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keyword = 'agency'  # Node that other models are not supported as of now\n",
    "model = BertForSequenceClassification.from_pretrained(f'xt0r3/aihype_{keyword}-vs-rest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf84476-8157-4603-aeab-30b3659f7687",
   "metadata": {},
   "source": [
    "## Adding input processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc79e0ce-8250-4ee0-a85b-4d0da5c7744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8b01703-effa-461b-9383-c4f76145e884",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = TextClassificationPipeline(\n",
    "    model=model, tokenizer=tokenizer, top_k=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8010627c-6682-48bc-a6d0-ebadcfb25773",
   "metadata": {},
   "source": [
    "## Define prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e5fa856-040a-4064-9660-0147133d00ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that keyword is defined above when loading the model\n",
    "decode = defaultdict(lambda: 'Undefined label', {\n",
    "    'LABEL_0': f'no {keyword}',\n",
    "    'LABEL_1': f'{keyword} present',\n",
    "})\n",
    "\n",
    "def predict(text):\n",
    "    pred_dict = dict()\n",
    "    preds = pipe(text)[0]\n",
    "    preds = {\n",
    "        k: v\n",
    "        for d in map(lambda pred: {decode[pred[\"label\"]]: pred[\"score\"]}, preds)\n",
    "        for k, v in d.items()\n",
    "    }\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42247f4c-7767-459b-bb79-dcbbccf6c198",
   "metadata": {},
   "source": [
    "## Playing around with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83a36ce0-cd78-4733-ba45-6568edc91ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A new vision of artificial intelligence for the people',\n",
       " 'The gig workers fighting back against the algorithms',\n",
       " 'How the AI industry profits from catastrophe',\n",
       " 'Artificial intelligence is creating a new colonial world order']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0e2664c-fa01-48c2-bff9-d701f21994fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline: A new vision of artificial intelligence for the people\n",
      "Prediction: {'no agency': 0.9999657869338989, 'agency present': 3.424444730626419e-05}\n",
      "Classification: NO AGENCY\n",
      "\n",
      "Headline: The gig workers fighting back against the algorithms\n",
      "Prediction: {'agency present': 0.9999499320983887, 'no agency': 5.0033140723826364e-05}\n",
      "Classification: AGENCY PRESENT\n",
      "\n",
      "Headline: How the AI industry profits from catastrophe\n",
      "Prediction: {'no agency': 0.99996018409729, 'agency present': 3.9774677134118974e-05}\n",
      "Classification: NO AGENCY\n",
      "\n",
      "Headline: Artificial intelligence is creating a new colonial world order\n",
      "Prediction: {'no agency': 0.9999510049819946, 'agency present': 4.896329119219445e-05}\n",
      "Classification: NO AGENCY\n",
      "\n",
      "Headline: South Africa’s private surveillance machine is fueling a digital apartheid\n",
      "Prediction: {'agency present': 0.9999490976333618, 'no agency': 5.091609273222275e-05}\n",
      "Classification: AGENCY PRESENT\n",
      "\n",
      "Headline: We built a database to understand the China Initiative. Then the government changed its records.\n",
      "Prediction: {'no agency': 0.9999632835388184, 'agency present': 3.6669352994067594e-05}\n",
      "Classification: NO AGENCY\n",
      "\n",
      "Headline: The US crackdown on Chinese economic espionage is a mess. We have the data to show it.\n",
      "Prediction: {'no agency': 0.9999622106552124, 'agency present': 3.782059138757177e-05}\n",
      "Classification: NO AGENCY\n",
      "\n",
      "Headline: How Facebook and Google fund global misinformation\n",
      "Prediction: {'no agency': 0.9999648332595825, 'agency present': 3.51550588675309e-05}\n",
      "Classification: NO AGENCY\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    preds = predict(dataset[i])\n",
    "    print(f'''Headline: {dataset[i]}\n",
    "Prediction: {preds}\n",
    "Classification: {max(preds, key=preds.get).upper()}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e518ec8c-a7cc-4d95-bb8f-84052be74b44",
   "metadata": {},
   "source": [
    "## Tokenization example \n",
    "This chapter shows an example of tokenization so that you can do data classification easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb36ad01-f6ce-4fa2-80b7-63a8b2489a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'new',\n",
       " 'vision',\n",
       " 'of',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'for',\n",
       " 'the',\n",
       " 'people']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK tokenizer for human-understood words\n",
    "word_tokenize(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6cd0f6a7-d8ab-46bc-a422-c4e57af3a51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['infinitesimal']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('infinitesimal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da77b87f-0b89-465a-a9ac-fa3941d9072c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'new',\n",
       " 'vision',\n",
       " 'of',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'for',\n",
       " 'the',\n",
       " 'people']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT tokeznier for subwords the model pays attention to. \n",
    "# This does not only find words, but also splits some long words to smaller subwords.\n",
    "tokenizer.tokenize(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8df07d56-da44-4b3e-bb80-357cf67b9356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['infinite', '##si', '##mal']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('infinitesimal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bfa4a4-c3a1-443a-8ab5-bea39eef725d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
