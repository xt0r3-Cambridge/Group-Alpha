{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16b24ad-1cef-4cef-b569-164a373eaaff",
   "metadata": {},
   "source": [
    "# Multi-label Classifier\n",
    "\n",
    "This notebook implements a multi-label classifier that fine-tunes a BERT model to tell if a sentence contains problematic metaphors\n",
    "\n",
    "<div hidden>\n",
    "TODO: add extend data3/data.json with better data in the same format that actually makes sense.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c329b4-dc31-4cd0-8058-b59015328184",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f0b50b-c9ff-45b5-979a-c2fa5634ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -Uqq\n",
    "!pip install sklearn -Uqq\n",
    "!pip install datasets -Uqq\n",
    "!pip install torch -Uqq\n",
    "!pip install numpy -Uqq\n",
    "!pip install evaluate -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d01c2eb7-f752-4628-b88d-48576c6461ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90809f9-9616-48f5-9631-68d270cbcf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"aihype_multi-label-bert\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ae221-1bbc-4a4a-8a1e-0feb9d17b57e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "898408be-0c15-4e58-9e7e-97fffccb4797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-766ccb02cf791abc\n",
      "Found cached dataset json (/home/xt0r3/.cache/huggingface/datasets/json/default-766ccb02cf791abc/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee33f1bc03a428b897c1ae227779ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'agency', 'humanComparison', 'hyperbole', 'historyComparison', 'unjustClaims', 'deepSounding', 'sceptics', 'deEmphasize', 'performanceNumber', 'inscrutable', 'objective'],\n",
       "        num_rows: 329\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"data3/data.json\", field=\"data\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b214393b-11e3-454e-b21f-a05ee38fefbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['A new vision of artificial intelligence for the people',\n",
       "  'The gig workers fighting back against the algorithms',\n",
       "  'How the AI industry profits from catastrophe'],\n",
       " 'agency': [False, True, False],\n",
       " 'humanComparison': [False, True, False],\n",
       " 'hyperbole': [False, True, True],\n",
       " 'historyComparison': [False, False, False],\n",
       " 'unjustClaims': [False, False, False],\n",
       " 'deepSounding': [False, False, False],\n",
       " 'sceptics': [False, False, False],\n",
       " 'deEmphasize': [False, False, False],\n",
       " 'performanceNumber': [False, False, False],\n",
       " 'inscrutable': [False, False, False],\n",
       " 'objective': [False, False, False]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce458fc2-c419-4b3c-b4aa-14571f7467e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agency',\n",
       " 'humanComparison',\n",
       " 'hyperbole',\n",
       " 'historyComparison',\n",
       " 'unjustClaims',\n",
       " 'deepSounding',\n",
       " 'sceptics',\n",
       " 'deEmphasize',\n",
       " 'performanceNumber',\n",
       " 'inscrutable',\n",
       " 'objective']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for label in dataset[\"train\"].features.keys() if label not in [\"text\"]]\n",
    "id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514633be-491c-4f3c-a884-8fa657c17ce1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocess Data, Create Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9348e944-7ebd-4d28-ae74-a6b2c2f116fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.class_encode_column(\"label\")\n",
    "dataset = dataset[\"train\"].train_test_split(\n",
    "    test_size=0.2\n",
    ")  # , stratify_by_column=\"label\")\n",
    "#\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f31d1c-4a03-497d-955b-0cffa75d0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    # take a batch of texts\n",
    "    text = examples[\"text\"]\n",
    "    # encode them\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "    # add labels\n",
    "    labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "    # create numpy array of shape (batch_size, num_labels)\n",
    "    labels_matrix = np.zeros((len(text), len(labels)))\n",
    "    # fill numpy array\n",
    "    for idx, label in enumerate(labels):\n",
    "        labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "    encoding[\"labels\"] = labels_matrix.tolist()\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbb35d3e-97ea-467f-87ca-da0c6d8700b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c366a7fe594ff18c179f8c7a206e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d895ef58e66f41bda50260c2b390a0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(\n",
    "    preprocess_data, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c1231d-e2ba-4731-9256-be6888f980c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Verify dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01929557-8708-49d0-8377-8a3dcce01e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "example = encoded_dataset[\"train\"][0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31f53959-845d-4073-8f1d-5aad8ac222eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] How Apple personalizes Siri without hoovering up your data [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8092ca8e-46fd-43ad-a7be-88e94055b695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0f5e3c6-3241-4bc4-8774-a2ebc9b6fc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2label[idx] for idx, label in enumerate(example[\"labels\"]) if label == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b50059a9-1c72-4ce8-b52f-be0992437953",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bae895c-92b0-4238-94e7-4310d103d4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 263\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 66\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a7006ec-7e97-48df-bfb8-0c4c03f3d82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train'][0]['labels'].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "217e03ea-960f-4f4c-9d22-9cd86f9aba91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1731,  7302,  2357,  9534,  2203,  1182,  1443, 16358,  5909,\n",
       "         1158,  1146,  1240,  2233,   102,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9820a5-b315-4fc2-b01b-8b57ce1f377a",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "595efe69-78aa-4cd4-938c-8d65c88e0bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# use_fast uses fast tokenizers backed by rust. Remove it if it causes errors\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=len(labels),\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e970785-d836-4ffb-941e-d9c271ef6ce2",
   "metadata": {},
   "source": [
    "### Verify data-model interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "999979a2-ee24-45e1-9433-7b9b001c9b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.7275, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[-0.3235, -0.0577,  0.1060, -0.0252,  0.2660, -0.2880, -0.2013,  0.2472,\n",
       "          0.1995,  0.2474,  0.4223]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forward pass\n",
    "outputs = model(input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0), labels=encoded_dataset['train'][0]['labels'].unsqueeze(0))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24902ec3-5c93-48d6-a4ec-980cd327eee2",
   "metadata": {},
   "source": [
    "## Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a724497-7a15-48f8-ba60-95f7d5f0c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {\"f1\": f1_micro_average, \"roc_auc\": roc_auc, \"accuracy\": accuracy}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    result = multi_label_metrics(predictions=preds, labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716eedb7-f61e-471a-bd17-9750bfc07bdc",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fc962c2-e601-4f0f-9261-4d2cd1af5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c435f5f9-bdff-4f4d-9973-3c72b8f8d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    MODEL_NAME,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    # push_to_hub=True,  # TODO: enable once model seems good\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa6bc9b3-2cde-4db2-b6d3-9bc871cc9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a016ae0c-3195-4fe6-950a-4a0a6dda6436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xt0r3/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 263\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3945\n",
      "  Number of trainable parameters = 108318731\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3945' max='3945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3945/3945 05:02, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.226728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.606061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.217300</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.606061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.217300</td>\n",
       "      <td>0.209764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.606061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.201446</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.509399</td>\n",
       "      <td>0.606061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.207337</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.541272</td>\n",
       "      <td>0.590909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.115000</td>\n",
       "      <td>0.211075</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.567999</td>\n",
       "      <td>0.484848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.115000</td>\n",
       "      <td>0.229822</td>\n",
       "      <td>0.196721</td>\n",
       "      <td>0.558600</td>\n",
       "      <td>0.515152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.082600</td>\n",
       "      <td>0.224676</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.539066</td>\n",
       "      <td>0.560606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.082600</td>\n",
       "      <td>0.244213</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.559335</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.063200</td>\n",
       "      <td>0.255359</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.527462</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.063200</td>\n",
       "      <td>0.248496</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.591208</td>\n",
       "      <td>0.530303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>0.255488</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.569469</td>\n",
       "      <td>0.515152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>0.259663</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.569469</td>\n",
       "      <td>0.515152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.263757</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.570205</td>\n",
       "      <td>0.530303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.265755</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.537596</td>\n",
       "      <td>0.530303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-finetuned-multi-label-ai-hype/checkpoint-263\n",
      "Configuration saved in bert-finetuned-multi-label-ai-hype/checkpoint-263/config.json\n",
      "Model weights saved in bert-finetuned-multi-label-ai-hype/checkpoint-263/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-finetuned-multi-label-ai-hype/checkpoint-526\n",
      "Configuration saved in bert-finetuned-multi-label-ai-hype/checkpoint-526/config.json\n",
      "Model weights saved in bert-finetuned-multi-label-ai-hype/checkpoint-526/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-finetuned-multi-label-ai-hype/checkpoint-789\n",
      "Configuration saved in bert-finetuned-multi-label-ai-hype/checkpoint-789/config.json\n",
      "Model weights saved in bert-finetuned-multi-label-ai-hype/checkpoint-789/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-finetuned-multi-label-ai-hype/checkpoint-1052\n",
      "Configuration saved in bert-finetuned-multi-label-ai-hype/checkpoint-1052/config.json\n",
      "Model weights saved in bert-finetuned-multi-label-ai-hype/checkpoint-1052/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-finetuned-multi-label-ai-hype/checkpoint-1315\n",
      "Configuration saved in bert-finetuned-multi-label-ai-hype/checkpoint-1315/config.json\n",
      "Model weights saved in bert-finetuned-multi-label-ai-hype/checkpoint-1315/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-finetuned-multi-label-ai-hype/checkpoint-1578\n",
      "Configuration saved in bert-finetuned-multi-label-ai-hype/checkpoint-1578/config.json\n",
      "Model weights saved in bert-finetuned-multi-label-ai-hype/checkpoint-1578/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-finetuned-multi-label-ai-hype/checkpoint-1841\n",
      "Configuration saved in bert-finetuned-multi-label-ai-hype/checkpoint-1841/config.json\n",
      "Model weights saved in bert-finetuned-multi-label-ai-hype/checkpoint-1841/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-finetuned-multi-label-ai-hype/checkpoint-2104\n",
      "Configuration saved in bert-finetuned-multi-label-ai-hype/checkpoint-2104/config.json\n",
      "Model weights saved in bert-finetuned-multi-label-ai-hype/checkpoint-2104/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-finetuned-multi-label-ai-hype/checkpoint-2367\n",
      "Configuration saved in bert-finetuned-multi-label-ai-hype/checkpoint-2367/config.json\n",
      "Model weights saved in bert-finetuned-multi-label-ai-hype/checkpoint-2367/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-finetuned-multi-label-ai-hype/checkpoint-2630\n",
      "Configuration saved in bert-finetuned-multi-label-ai-hype/checkpoint-2630/config.json\n",
      "Model weights saved in bert-finetuned-multi-label-ai-hype/checkpoint-2630/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-finetuned-multi-label-ai-hype/checkpoint-2893\n",
      "Configuration saved in bert-finetuned-multi-label-ai-hype/checkpoint-2893/config.json\n",
      "Model weights saved in bert-finetuned-multi-label-ai-hype/checkpoint-2893/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-finetuned-multi-label-ai-hype/checkpoint-3156\n",
      "Configuration saved in bert-finetuned-multi-label-ai-hype/checkpoint-3156/config.json\n",
      "Model weights saved in bert-finetuned-multi-label-ai-hype/checkpoint-3156/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-finetuned-multi-label-ai-hype/checkpoint-3419\n",
      "Configuration saved in bert-finetuned-multi-label-ai-hype/checkpoint-3419/config.json\n",
      "Model weights saved in bert-finetuned-multi-label-ai-hype/checkpoint-3419/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-finetuned-multi-label-ai-hype/checkpoint-3682\n",
      "Configuration saved in bert-finetuned-multi-label-ai-hype/checkpoint-3682/config.json\n",
      "Model weights saved in bert-finetuned-multi-label-ai-hype/checkpoint-3682/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-finetuned-multi-label-ai-hype/checkpoint-3945\n",
      "Configuration saved in bert-finetuned-multi-label-ai-hype/checkpoint-3945/config.json\n",
      "Model weights saved in bert-finetuned-multi-label-ai-hype/checkpoint-3945/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from bert-finetuned-multi-label-ai-hype/checkpoint-2893 (score: 0.28125).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3945, training_loss=0.09761825000680747, metrics={'train_runtime': 303.6426, 'train_samples_per_second': 12.992, 'train_steps_per_second': 12.992, 'total_flos': 259514247317760.0, 'train_loss': 0.09761825000680747, 'epoch': 15.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b028df0-8be7-409e-a3ce-1c676eb1e9fa",
   "metadata": {},
   "source": [
    "## Upload the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "003bb89c-8ee0-4bc6-9316-cce5ebd5ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
