{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79688b38-ed00-4f35-b491-6a41e2433a6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# One-vs-Rest Classifier\n",
    "\n",
    "This notebook implements an one-vs-rest classifier that fine-tunes several BERT models to tell if a sentence contains problematic metaphors.\n",
    "\n",
    "<div hidden>\n",
    "TODO: add extend data3/data.json with better data in the same format that actually makes sense.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c329b4-dc31-4cd0-8058-b59015328184",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f0b50b-c9ff-45b5-979a-c2fa5634ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -Uqq\n",
    "!pip install sklearn -Uqq\n",
    "!pip install datasets -Uqq\n",
    "!pip install torch -Uqq\n",
    "!pip install numpy -Uqq\n",
    "!pip install evaluate -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d01c2eb7-f752-4628-b88d-48576c6461ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from transformers import (\n",
    "    AutoModelForNextSentencePrediction,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14c78698-ce72-41f7-96fb-111a4f1d3729",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"one-vs-rest-bert\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ae221-1bbc-4a4a-8a1e-0feb9d17b57e",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "898408be-0c15-4e58-9e7e-97fffccb4797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-db1d0de5b8eb1a4a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/xt0r3/.cache/huggingface/datasets/json/default-db1d0de5b8eb1a4a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c77bf4e9be4f0aaaa4ae80a36265d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99babb8d871640faaa924d997a2943d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/xt0r3/.cache/huggingface/datasets/json/default-db1d0de5b8eb1a4a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d18361efdf04662943b9a8938cb4d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 8284\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"data/data_unlab_clean.json\", field=\"data\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b214393b-11e3-454e-b21f-a05ee38fefbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [['By',\n",
       "   'Australian',\n",
       "   'Associated',\n",
       "   'Press',\n",
       "   'Published',\n",
       "   ':',\n",
       "   '06:13',\n",
       "   ',',\n",
       "   '23',\n",
       "   'April',\n",
       "   '2019',\n",
       "   '|',\n",
       "   'Updated',\n",
       "   ':',\n",
       "   '06:13',\n",
       "   ',',\n",
       "   '23',\n",
       "   'April',\n",
       "   '2019',\n",
       "   'A',\n",
       "   'new',\n",
       "   'artificial',\n",
       "   'intelligence',\n",
       "   '(',\n",
       "   'AI',\n",
       "   ')',\n",
       "   'centre',\n",
       "   'of',\n",
       "   'excellence',\n",
       "   'will',\n",
       "   'be',\n",
       "   'tasked',\n",
       "   'with',\n",
       "   'preventing',\n",
       "   'another',\n",
       "   '``',\n",
       "   'robodebt',\n",
       "   \"''\",\n",
       "   'disaster',\n",
       "   'if',\n",
       "   'Labor',\n",
       "   'wins',\n",
       "   'the',\n",
       "   'federal',\n",
       "   'election',\n",
       "   '.',\n",
       "   'The',\n",
       "   'AI',\n",
       "   'centre',\n",
       "   'will',\n",
       "   'be',\n",
       "   'based',\n",
       "   'in',\n",
       "   'Melbourne',\n",
       "   'with',\n",
       "   'a',\n",
       "   '$',\n",
       "   '3',\n",
       "   'million',\n",
       "   'commitment',\n",
       "   'from',\n",
       "   'Labor',\n",
       "   'and',\n",
       "   '$',\n",
       "   '1',\n",
       "   'million',\n",
       "   'from',\n",
       "   'the',\n",
       "   'Victorian',\n",
       "   'government',\n",
       "   '.',\n",
       "   'Labor',\n",
       "   \"'s\",\n",
       "   'digital',\n",
       "   'technology',\n",
       "   'spokesman',\n",
       "   'Ed',\n",
       "   'Husic',\n",
       "   'said',\n",
       "   'the',\n",
       "   'centre',\n",
       "   'will',\n",
       "   'devise',\n",
       "   'a',\n",
       "   'strategy',\n",
       "   'for',\n",
       "   'ethical',\n",
       "   'use',\n",
       "   'of',\n",
       "   'artificial',\n",
       "   'intelligence',\n",
       "   'in',\n",
       "   'Australian',\n",
       "   'government',\n",
       "   'and',\n",
       "   'business',\n",
       "   '.',\n",
       "   '``',\n",
       "   'The',\n",
       "   'application',\n",
       "   'of',\n",
       "   'tech',\n",
       "   'within',\n",
       "   'government',\n",
       "   'and',\n",
       "   'decision-making',\n",
       "   'processes',\n",
       "   'is',\n",
       "   'a',\n",
       "   'smart',\n",
       "   'thing',\n",
       "   '-',\n",
       "   'if',\n",
       "   'it',\n",
       "   'can',\n",
       "   'make',\n",
       "   'decisions',\n",
       "   'quicker',\n",
       "   ',',\n",
       "   'more',\n",
       "   'efficiently',\n",
       "   ',',\n",
       "   'great',\n",
       "   ',',\n",
       "   \"''\",\n",
       "   'he',\n",
       "   'told',\n",
       "   'reporters',\n",
       "   'in',\n",
       "   'Melbourne',\n",
       "   'on',\n",
       "   'Tuesday',\n",
       "   '.',\n",
       "   '``',\n",
       "   'But',\n",
       "   'systems',\n",
       "   'have',\n",
       "   'to',\n",
       "   'get',\n",
       "   'designed',\n",
       "   'by',\n",
       "   'someone',\n",
       "   ',',\n",
       "   'you',\n",
       "   'have',\n",
       "   'to',\n",
       "   'think',\n",
       "   'through',\n",
       "   'the',\n",
       "   'end-to-end',\n",
       "   'process',\n",
       "   '.',\n",
       "   \"''\",\n",
       "   'The',\n",
       "   'Centrelink',\n",
       "   '``',\n",
       "   'robodebt',\n",
       "   \"''\",\n",
       "   'system',\n",
       "   'attracted',\n",
       "   'severe',\n",
       "   'criticism',\n",
       "   'for',\n",
       "   'demanding',\n",
       "   'payment',\n",
       "   'for',\n",
       "   'old',\n",
       "   'debts',\n",
       "   ',',\n",
       "   'despite',\n",
       "   'many',\n",
       "   'people',\n",
       "   'not',\n",
       "   'actually',\n",
       "   'owing',\n",
       "   'money',\n",
       "   'to',\n",
       "   'the',\n",
       "   'government',\n",
       "   '.',\n",
       "   'Some',\n",
       "   'had',\n",
       "   'payments',\n",
       "   'cut',\n",
       "   'unfairly',\n",
       "   ',',\n",
       "   'while',\n",
       "   'others',\n",
       "   'went',\n",
       "   'through',\n",
       "   'a',\n",
       "   'bureaucratic',\n",
       "   'nightmare',\n",
       "   'trying',\n",
       "   'to',\n",
       "   'get',\n",
       "   'false',\n",
       "   'debts',\n",
       "   'removed',\n",
       "   '.',\n",
       "   '``',\n",
       "   'Robodebt',\n",
       "   'is',\n",
       "   'a',\n",
       "   'classic',\n",
       "   'case',\n",
       "   'of',\n",
       "   'where',\n",
       "   'a',\n",
       "   'system',\n",
       "   'was',\n",
       "   'set',\n",
       "   'up',\n",
       "   'where',\n",
       "   'the',\n",
       "   'thinking',\n",
       "   'had',\n",
       "   \"n't\",\n",
       "   'been',\n",
       "   'done',\n",
       "   'early',\n",
       "   'on',\n",
       "   ',',\n",
       "   \"''\",\n",
       "   'Mr',\n",
       "   'Husic',\n",
       "   'said',\n",
       "   '.',\n",
       "   '``',\n",
       "   'What',\n",
       "   'would',\n",
       "   'happen',\n",
       "   'if',\n",
       "   'a',\n",
       "   'decision',\n",
       "   'went',\n",
       "   'wrong',\n",
       "   'and',\n",
       "   'how',\n",
       "   'would',\n",
       "   'an',\n",
       "   'ordinary',\n",
       "   'person',\n",
       "   'be',\n",
       "   'able',\n",
       "   'to',\n",
       "   'make',\n",
       "   'contact',\n",
       "   'with',\n",
       "   'someone',\n",
       "   'to',\n",
       "   'get',\n",
       "   'that',\n",
       "   'corrected',\n",
       "   '?',\n",
       "   \"''\",\n",
       "   'Mr',\n",
       "   'Husic',\n",
       "   'said',\n",
       "   'the',\n",
       "   'centre',\n",
       "   \"'s\",\n",
       "   'mission',\n",
       "   'will',\n",
       "   'be',\n",
       "   'to',\n",
       "   'help',\n",
       "   'avoid',\n",
       "   'the',\n",
       "   '``',\n",
       "   'dumb',\n",
       "   'decisions',\n",
       "   \"''\",\n",
       "   'that',\n",
       "   'occur',\n",
       "   'with',\n",
       "   'badly-designed',\n",
       "   'AI',\n",
       "   ',',\n",
       "   'and',\n",
       "   'he',\n",
       "   'flagged',\n",
       "   'laws',\n",
       "   'to',\n",
       "   'tackle',\n",
       "   'bad',\n",
       "   'behaviour',\n",
       "   'from',\n",
       "   'businesses',\n",
       "   '.',\n",
       "   '``',\n",
       "   'You',\n",
       "   'just',\n",
       "   'ca',\n",
       "   \"n't\",\n",
       "   'expect',\n",
       "   'that',\n",
       "   'if',\n",
       "   'you',\n",
       "   'have',\n",
       "   'the',\n",
       "   'ethical',\n",
       "   'framework',\n",
       "   'in',\n",
       "   'place',\n",
       "   'that',\n",
       "   'businesses',\n",
       "   'will',\n",
       "   'always',\n",
       "   'make',\n",
       "   'the',\n",
       "   'right',\n",
       "   'decision',\n",
       "   ',',\n",
       "   \"''\",\n",
       "   'he',\n",
       "   'said',\n",
       "   '.',\n",
       "   '``',\n",
       "   'Law',\n",
       "   'will',\n",
       "   'have',\n",
       "   'to',\n",
       "   'be',\n",
       "   'there',\n",
       "   'in',\n",
       "   'those',\n",
       "   'cases',\n",
       "   'where',\n",
       "   'things',\n",
       "   'do',\n",
       "   \"n't\",\n",
       "   'operate',\n",
       "   'the',\n",
       "   'way',\n",
       "   'you',\n",
       "   'want',\n",
       "   'it',\n",
       "   'to',\n",
       "   '.',\n",
       "   \"''\",\n",
       "   'Sorry',\n",
       "   'we',\n",
       "   'are',\n",
       "   'not',\n",
       "   'currently',\n",
       "   'accepting',\n",
       "   'comments',\n",
       "   'on',\n",
       "   'this',\n",
       "   'article',\n",
       "   '.',\n",
       "   'Published',\n",
       "   'by',\n",
       "   'Associated',\n",
       "   'Newspapers',\n",
       "   'Ltd',\n",
       "   'Part',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Daily',\n",
       "   'Mail',\n",
       "   ',',\n",
       "   'The',\n",
       "   'Mail',\n",
       "   'on',\n",
       "   'Sunday',\n",
       "   '&',\n",
       "   'Metro',\n",
       "   'Media',\n",
       "   'Group'],\n",
       "  ['By',\n",
       "   'Australian',\n",
       "   'Associated',\n",
       "   'Press',\n",
       "   'Published',\n",
       "   ':',\n",
       "   '00:58',\n",
       "   ',',\n",
       "   '7',\n",
       "   'December',\n",
       "   '2020',\n",
       "   '|',\n",
       "   'Updated',\n",
       "   ':',\n",
       "   '00:58',\n",
       "   ',',\n",
       "   '7',\n",
       "   'December',\n",
       "   '2020',\n",
       "   'Australian',\n",
       "   'researchers',\n",
       "   'have',\n",
       "   'developed',\n",
       "   'an',\n",
       "   'artificial',\n",
       "   'intelligence',\n",
       "   'device',\n",
       "   'that',\n",
       "   'can',\n",
       "   'monitor',\n",
       "   'and',\n",
       "   'help',\n",
       "   'prevent',\n",
       "   'seizures',\n",
       "   'in',\n",
       "   'patients',\n",
       "   'recovering',\n",
       "   'from',\n",
       "   'traumatic',\n",
       "   'brain',\n",
       "   'injuries',\n",
       "   '.',\n",
       "   'Traumatic',\n",
       "   'brain',\n",
       "   'injuries',\n",
       "   'affect',\n",
       "   'more',\n",
       "   'than',\n",
       "   '69',\n",
       "   'million',\n",
       "   'people',\n",
       "   'across',\n",
       "   'the',\n",
       "   'globe',\n",
       "   ',',\n",
       "   'including',\n",
       "   '700,000',\n",
       "   'Australians',\n",
       "   '.',\n",
       "   'One-in-three',\n",
       "   'of',\n",
       "   'those',\n",
       "   'are',\n",
       "   'likely',\n",
       "   'to',\n",
       "   'develop',\n",
       "   'chronic',\n",
       "   'epilepsy',\n",
       "   'as',\n",
       "   'a',\n",
       "   'result',\n",
       "   '.',\n",
       "   'The',\n",
       "   'device',\n",
       "   'developed',\n",
       "   'by',\n",
       "   'Australia',\n",
       "   \"'s\",\n",
       "   'national',\n",
       "   'science',\n",
       "   'agency',\n",
       "   ',',\n",
       "   'CSIRO',\n",
       "   ',',\n",
       "   'uses',\n",
       "   'a',\n",
       "   'form',\n",
       "   'of',\n",
       "   'artificial',\n",
       "   'intelligence',\n",
       "   'to',\n",
       "   'track',\n",
       "   'brain',\n",
       "   'swelling',\n",
       "   'and',\n",
       "   'detect',\n",
       "   'even',\n",
       "   'the',\n",
       "   'smallest',\n",
       "   'seizures',\n",
       "   '.',\n",
       "   'It',\n",
       "   'then',\n",
       "   'transmits',\n",
       "   'the',\n",
       "   'data',\n",
       "   'to',\n",
       "   'the',\n",
       "   'patient',\n",
       "   \"'s\",\n",
       "   'doctors',\n",
       "   '.',\n",
       "   '``',\n",
       "   'These',\n",
       "   'seizures',\n",
       "   'are',\n",
       "   'often',\n",
       "   'difficult',\n",
       "   'to',\n",
       "   'detect',\n",
       "   ',',\n",
       "   'with',\n",
       "   'current',\n",
       "   'monitoring',\n",
       "   'techniques',\n",
       "   'only',\n",
       "   'able',\n",
       "   'to',\n",
       "   'be',\n",
       "   'used',\n",
       "   'in',\n",
       "   'a',\n",
       "   'hospital',\n",
       "   'using',\n",
       "   'bulky',\n",
       "   'devices',\n",
       "   'for',\n",
       "   'less',\n",
       "   'than',\n",
       "   '24',\n",
       "   'hours',\n",
       "   ',',\n",
       "   'providing',\n",
       "   'a',\n",
       "   'brief',\n",
       "   'snapshot',\n",
       "   'of',\n",
       "   'brain',\n",
       "   'activity',\n",
       "   'during',\n",
       "   'that',\n",
       "   'time',\n",
       "   'only',\n",
       "   ',',\n",
       "   \"''\",\n",
       "   'researcher',\n",
       "   'Umut',\n",
       "   'Guvenc',\n",
       "   'said',\n",
       "   '.',\n",
       "   '``',\n",
       "   'This',\n",
       "   'new',\n",
       "   'method',\n",
       "   'can',\n",
       "   'continuously',\n",
       "   'monitor',\n",
       "   'brain',\n",
       "   'activity',\n",
       "   'wirelessly',\n",
       "   ',',\n",
       "   'allowing',\n",
       "   'the',\n",
       "   'patient',\n",
       "   'to',\n",
       "   'be',\n",
       "   'mobile',\n",
       "   ',',\n",
       "   'comfortable',\n",
       "   'and',\n",
       "   'more',\n",
       "   'socially',\n",
       "   'active',\n",
       "   ',',\n",
       "   \"''\",\n",
       "   'Dr',\n",
       "   'Guvenc',\n",
       "   'said',\n",
       "   '.',\n",
       "   'The',\n",
       "   'new',\n",
       "   'device',\n",
       "   ',',\n",
       "   'which',\n",
       "   'researchers',\n",
       "   'are',\n",
       "   'developing',\n",
       "   'in',\n",
       "   'a',\n",
       "   '``',\n",
       "   'smart',\n",
       "   'helmet',\n",
       "   \"''\",\n",
       "   ',',\n",
       "   'is',\n",
       "   'specifically',\n",
       "   'designed',\n",
       "   'for',\n",
       "   'those',\n",
       "   'recovering',\n",
       "   'from',\n",
       "   'brain',\n",
       "   'surgery',\n",
       "   'and',\n",
       "   'strokes',\n",
       "   '.',\n",
       "   'Many',\n",
       "   'people',\n",
       "   'who',\n",
       "   'suffer',\n",
       "   'a',\n",
       "   'traumatic',\n",
       "   'brain',\n",
       "   'injury',\n",
       "   'have',\n",
       "   'part',\n",
       "   'of',\n",
       "   'their',\n",
       "   'skull',\n",
       "   'temporarily',\n",
       "   'removed',\n",
       "   'to',\n",
       "   'relieve',\n",
       "   'pressure',\n",
       "   'on',\n",
       "   'the',\n",
       "   'brain',\n",
       "   '.',\n",
       "   'Long',\n",
       "   'term',\n",
       "   ',',\n",
       "   'researchers',\n",
       "   'hope',\n",
       "   'the',\n",
       "   'data',\n",
       "   'collected',\n",
       "   'by',\n",
       "   'their',\n",
       "   'smart',\n",
       "   'helmet',\n",
       "   'will',\n",
       "   'help',\n",
       "   'narrow',\n",
       "   'down',\n",
       "   'the',\n",
       "   'best',\n",
       "   'time',\n",
       "   'to',\n",
       "   'reconstruct',\n",
       "   'people',\n",
       "   \"'s\",\n",
       "   'skulls',\n",
       "   '.',\n",
       "   '``',\n",
       "   'The',\n",
       "   'combination',\n",
       "   'of',\n",
       "   'brain',\n",
       "   'swelling',\n",
       "   ',',\n",
       "   'surgery',\n",
       "   'timing',\n",
       "   'and',\n",
       "   'patient',\n",
       "   'outcome',\n",
       "   'data',\n",
       "   'will',\n",
       "   'enable',\n",
       "   'further',\n",
       "   'study',\n",
       "   'on',\n",
       "   'the',\n",
       "   'ideal',\n",
       "   'time',\n",
       "   'to',\n",
       "   'perform',\n",
       "   'a',\n",
       "   'reconstructive',\n",
       "   'cranioplasty',\n",
       "   'to',\n",
       "   'achieve',\n",
       "   'the',\n",
       "   'best',\n",
       "   'patient',\n",
       "   'outcome',\n",
       "   '-',\n",
       "   'research',\n",
       "   'that',\n",
       "   'will',\n",
       "   'ultimately',\n",
       "   'influence',\n",
       "   'future',\n",
       "   'medical',\n",
       "   'decision',\n",
       "   ',',\n",
       "   \"''\",\n",
       "   'senior',\n",
       "   'research',\n",
       "   'engineer',\n",
       "   'Peter',\n",
       "   'Marendy',\n",
       "   'said',\n",
       "   '.',\n",
       "   'Sorry',\n",
       "   'we',\n",
       "   'are',\n",
       "   'not',\n",
       "   'currently',\n",
       "   'accepting',\n",
       "   'comments',\n",
       "   'on',\n",
       "   'this',\n",
       "   'article',\n",
       "   '.',\n",
       "   'Published',\n",
       "   'by',\n",
       "   'Associated',\n",
       "   'Newspapers',\n",
       "   'Ltd',\n",
       "   'Part',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Daily',\n",
       "   'Mail',\n",
       "   ',',\n",
       "   'The',\n",
       "   'Mail',\n",
       "   'on',\n",
       "   'Sunday',\n",
       "   '&',\n",
       "   'Metro',\n",
       "   'Media',\n",
       "   'Group'],\n",
       "  ['By',\n",
       "   'Press',\n",
       "   'Association',\n",
       "   'Published',\n",
       "   ':',\n",
       "   '00:03',\n",
       "   ',',\n",
       "   '16',\n",
       "   'May',\n",
       "   '2019',\n",
       "   '|',\n",
       "   'Updated',\n",
       "   ':',\n",
       "   '00:03',\n",
       "   ',',\n",
       "   '16',\n",
       "   'May',\n",
       "   '2019',\n",
       "   'The',\n",
       "   'Government',\n",
       "   'has',\n",
       "   'confirmed',\n",
       "   'the',\n",
       "   'line-up',\n",
       "   'of',\n",
       "   'its',\n",
       "   'new',\n",
       "   'Artificial',\n",
       "   'Intelligence',\n",
       "   '(',\n",
       "   'AI',\n",
       "   ')',\n",
       "   'Council',\n",
       "   ',',\n",
       "   'the',\n",
       "   'independent',\n",
       "   'committee',\n",
       "   'that',\n",
       "   'will',\n",
       "   'discuss',\n",
       "   'how',\n",
       "   'to',\n",
       "   'promote',\n",
       "   'the',\n",
       "   'adoption',\n",
       "   'and',\n",
       "   'ethical',\n",
       "   'use',\n",
       "   'of',\n",
       "   'the',\n",
       "   'technology',\n",
       "   '.',\n",
       "   'Co-founder',\n",
       "   'of',\n",
       "   'Google-owned',\n",
       "   'AI',\n",
       "   'firm',\n",
       "   'DeepMind',\n",
       "   ',',\n",
       "   'Mustafa',\n",
       "   'Suleyman',\n",
       "   ',',\n",
       "   'AI',\n",
       "   'for',\n",
       "   'good',\n",
       "   'founder',\n",
       "   'Kriti',\n",
       "   'Sharma',\n",
       "   'and',\n",
       "   'Alan',\n",
       "   'Turing',\n",
       "   'Institute',\n",
       "   'director',\n",
       "   'and',\n",
       "   'chief',\n",
       "   'executive',\n",
       "   'Professor',\n",
       "   'Adrian',\n",
       "   'Smith',\n",
       "   'are',\n",
       "   'among',\n",
       "   'the',\n",
       "   'more',\n",
       "   'than',\n",
       "   '20',\n",
       "   'industry',\n",
       "   'experts',\n",
       "   'joining',\n",
       "   'the',\n",
       "   'panel',\n",
       "   '.',\n",
       "   'The',\n",
       "   'Government',\n",
       "   'had',\n",
       "   'previously',\n",
       "   'confirmed',\n",
       "   'the',\n",
       "   'creation',\n",
       "   'of',\n",
       "   'the',\n",
       "   'council',\n",
       "   'and',\n",
       "   'its',\n",
       "   'chair',\n",
       "   '–',\n",
       "   'Tabitha',\n",
       "   'Goldstaub',\n",
       "   ',',\n",
       "   'co-founder',\n",
       "   'of',\n",
       "   'online',\n",
       "   'platform',\n",
       "   'CognitionX',\n",
       "   ',',\n",
       "   'which',\n",
       "   'provides',\n",
       "   'companies',\n",
       "   'with',\n",
       "   'information',\n",
       "   'and',\n",
       "   'access',\n",
       "   'to',\n",
       "   'AI',\n",
       "   'experts',\n",
       "   '.',\n",
       "   'The',\n",
       "   'Government',\n",
       "   'said',\n",
       "   'the',\n",
       "   'council',\n",
       "   'will',\n",
       "   'look',\n",
       "   'into',\n",
       "   'identifying',\n",
       "   'and',\n",
       "   'overcoming',\n",
       "   'the',\n",
       "   'barriers',\n",
       "   'of',\n",
       "   'AI',\n",
       "   'adoption',\n",
       "   'in',\n",
       "   'society',\n",
       "   ',',\n",
       "   'including',\n",
       "   'issues',\n",
       "   'around',\n",
       "   'consumer',\n",
       "   'trust',\n",
       "   'and',\n",
       "   'protecting',\n",
       "   'consumer',\n",
       "   'data',\n",
       "   '.',\n",
       "   'Digital',\n",
       "   'Secretary',\n",
       "   'Jeremy',\n",
       "   'Wright',\n",
       "   'said',\n",
       "   'Britain',\n",
       "   'was',\n",
       "   '“',\n",
       "   'already',\n",
       "   'a',\n",
       "   'leading',\n",
       "   'authority',\n",
       "   'in',\n",
       "   'AI',\n",
       "   '”',\n",
       "   'and',\n",
       "   'the',\n",
       "   'council',\n",
       "   'would',\n",
       "   'help',\n",
       "   'continue',\n",
       "   'that',\n",
       "   'momentum',\n",
       "   '.',\n",
       "   '“',\n",
       "   'We',\n",
       "   'are',\n",
       "   'home',\n",
       "   'to',\n",
       "   'some',\n",
       "   'of',\n",
       "   'the',\n",
       "   'world',\n",
       "   '’',\n",
       "   's',\n",
       "   'finest',\n",
       "   'academic',\n",
       "   'institutions',\n",
       "   ',',\n",
       "   'landing',\n",
       "   'record',\n",
       "   'levels',\n",
       "   'of',\n",
       "   'investment',\n",
       "   'to',\n",
       "   'the',\n",
       "   'sector',\n",
       "   'and',\n",
       "   'attracting',\n",
       "   'the',\n",
       "   'best',\n",
       "   'global',\n",
       "   'tech',\n",
       "   'talent',\n",
       "   ',',\n",
       "   'but',\n",
       "   'we',\n",
       "   'must',\n",
       "   'not',\n",
       "   'be',\n",
       "   'complacent',\n",
       "   ',',\n",
       "   '”',\n",
       "   'he',\n",
       "   'said',\n",
       "   '.',\n",
       "   'A',\n",
       "   'person',\n",
       "   'shakes',\n",
       "   'hands',\n",
       "   'with',\n",
       "   'Shadow',\n",
       "   'Robot',\n",
       "   'Company´s',\n",
       "   'C5',\n",
       "   'Dexterous',\n",
       "   'Hand',\n",
       "   '(',\n",
       "   'Tim',\n",
       "   'Ireland/PA',\n",
       "   ')',\n",
       "   '“',\n",
       "   'Through',\n",
       "   'our',\n",
       "   'AI',\n",
       "   'Council',\n",
       "   'we',\n",
       "   'will',\n",
       "   'continue',\n",
       "   'this',\n",
       "   'momentum',\n",
       "   'by',\n",
       "   'leveraging',\n",
       "   'the',\n",
       "   'knowledge',\n",
       "   'of',\n",
       "   'experts',\n",
       "   'from',\n",
       "   'a',\n",
       "   'range',\n",
       "   'of',\n",
       "   'sectors',\n",
       "   'to',\n",
       "   'provide',\n",
       "   'leadership',\n",
       "   'on',\n",
       "   'the',\n",
       "   'best',\n",
       "   'use',\n",
       "   'and',\n",
       "   'adoption',\n",
       "   'of',\n",
       "   'artificial',\n",
       "   'intelligence',\n",
       "   'across',\n",
       "   'the',\n",
       "   'economy',\n",
       "   '.',\n",
       "   '“',\n",
       "   'Under',\n",
       "   'the',\n",
       "   'leadership',\n",
       "   'of',\n",
       "   'Tabitha',\n",
       "   'Goldstaub',\n",
       "   ',',\n",
       "   'the',\n",
       "   'Council',\n",
       "   'will',\n",
       "   'represent',\n",
       "   'the',\n",
       "   'UK',\n",
       "   'AI',\n",
       "   'Sector',\n",
       "   'on',\n",
       "   'the',\n",
       "   'international',\n",
       "   'stage',\n",
       "   'and',\n",
       "   'help',\n",
       "   'us',\n",
       "   'put',\n",
       "   'in',\n",
       "   'place',\n",
       "   'the',\n",
       "   'right',\n",
       "   'skills',\n",
       "   'and',\n",
       "   'practices',\n",
       "   'to',\n",
       "   'make',\n",
       "   'the',\n",
       "   'most',\n",
       "   'of',\n",
       "   'data-driven',\n",
       "   'technologies',\n",
       "   '.',\n",
       "   '”',\n",
       "   'Confirmation',\n",
       "   'of',\n",
       "   'the',\n",
       "   'council',\n",
       "   '’',\n",
       "   's',\n",
       "   'membership',\n",
       "   'comes',\n",
       "   'on',\n",
       "   'the',\n",
       "   'first',\n",
       "   'anniversary',\n",
       "   'of',\n",
       "   'the',\n",
       "   'AI',\n",
       "   'Sector',\n",
       "   'deal',\n",
       "   ',',\n",
       "   'a',\n",
       "   'billion-pound',\n",
       "   'joint',\n",
       "   'venture',\n",
       "   'between',\n",
       "   'the',\n",
       "   'Government',\n",
       "   'and',\n",
       "   'industry',\n",
       "   'to',\n",
       "   'try',\n",
       "   'and',\n",
       "   'push',\n",
       "   'the',\n",
       "   'UK',\n",
       "   'to',\n",
       "   'the',\n",
       "   'forefront',\n",
       "   'of',\n",
       "   'emerging',\n",
       "   'technology',\n",
       "   'such',\n",
       "   'as',\n",
       "   'AI',\n",
       "   '.',\n",
       "   '“',\n",
       "   'The',\n",
       "   'use',\n",
       "   'of',\n",
       "   'Artificial',\n",
       "   'Intelligence',\n",
       "   'is',\n",
       "   'becoming',\n",
       "   'integral',\n",
       "   'to',\n",
       "   'people',\n",
       "   '’',\n",
       "   's',\n",
       "   'everyday',\n",
       "   'lives',\n",
       "   ',',\n",
       "   'from',\n",
       "   'companies',\n",
       "   'protecting',\n",
       "   'their',\n",
       "   'customers',\n",
       "   'from',\n",
       "   'fraud',\n",
       "   'to',\n",
       "   'smart',\n",
       "   'devices',\n",
       "   'in',\n",
       "   'our',\n",
       "   'homes',\n",
       "   ',',\n",
       "   '”',\n",
       "   'Business',\n",
       "   'Secretary',\n",
       "   'Greg',\n",
       "   'Clark',\n",
       "   'said',\n",
       "   '.',\n",
       "   '“',\n",
       "   'The',\n",
       "   'outstanding',\n",
       "   'expertise',\n",
       "   'of',\n",
       "   'those',\n",
       "   'joining',\n",
       "   'our',\n",
       "   'new',\n",
       "   'AI',\n",
       "   'Council',\n",
       "   'will',\n",
       "   'be',\n",
       "   'invaluable',\n",
       "   'as',\n",
       "   'we',\n",
       "   'look',\n",
       "   'to',\n",
       "   'develop',\n",
       "   'this',\n",
       "   'ever-changing',\n",
       "   'industry',\n",
       "   'into',\n",
       "   'one',\n",
       "   'that',\n",
       "   'is',\n",
       "   'world-leading',\n",
       "   ',',\n",
       "   'attracting',\n",
       "   'the',\n",
       "   'brightest',\n",
       "   'and',\n",
       "   'best',\n",
       "   'to',\n",
       "   'work',\n",
       "   'in',\n",
       "   'new',\n",
       "   'highly-skilled',\n",
       "   'jobs',\n",
       "   '.',\n",
       "   '“',\n",
       "   'This',\n",
       "   'AI',\n",
       "   'Council',\n",
       "   'follows',\n",
       "   'our',\n",
       "   'ground-breaking',\n",
       "   'AI',\n",
       "   'Sector',\n",
       "   'Deal',\n",
       "   ',',\n",
       "   'and',\n",
       "   'is',\n",
       "   'a',\n",
       "   'key',\n",
       "   'part',\n",
       "   'of',\n",
       "   'our',\n",
       "   'modern',\n",
       "   'Industrial',\n",
       "   'Strategy',\n",
       "   '–',\n",
       "   'investing',\n",
       "   'now',\n",
       "   'to',\n",
       "   'secure',\n",
       "   'the',\n",
       "   'UK',\n",
       "   '’',\n",
       "   's',\n",
       "   'position',\n",
       "   'on',\n",
       "   'the',\n",
       "   'world',\n",
       "   'stage',\n",
       "   'in',\n",
       "   'these',\n",
       "   'cutting',\n",
       "   'edge',\n",
       "   'technologies',\n",
       "   'both',\n",
       "   'now',\n",
       "   'and',\n",
       "   'long',\n",
       "   'into',\n",
       "   'the',\n",
       "   'future',\n",
       "   '.',\n",
       "   '”',\n",
       "   'Sorry',\n",
       "   'we',\n",
       "   'are',\n",
       "   'not',\n",
       "   'currently',\n",
       "   'accepting',\n",
       "   'comments',\n",
       "   'on',\n",
       "   'this',\n",
       "   'article',\n",
       "   '.',\n",
       "   'Published',\n",
       "   'by',\n",
       "   'Associated',\n",
       "   'Newspapers',\n",
       "   'Ltd',\n",
       "   'Part',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Daily',\n",
       "   'Mail',\n",
       "   ',',\n",
       "   'The',\n",
       "   'Mail',\n",
       "   'on',\n",
       "   'Sunday',\n",
       "   '&',\n",
       "   'Metro',\n",
       "   'Media',\n",
       "   'Group']]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce458fc2-c419-4b3c-b4aa-14571f7467e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514633be-491c-4f3c-a884-8fa657c17ce1",
   "metadata": {},
   "source": [
    "## Preprocess Data, Create Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9348e944-7ebd-4d28-ae74-a6b2c2f116fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 6627\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1657\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c5496177-a13e-401d-8d1b-2c0be9f63ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-db1d0de5b8eb1a4a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-bde19c90e03502da.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-db1d0de5b8eb1a4a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-066a1ebdde2c570e.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': \"By Alex Hammer For Dailymail.Com Published : 06:12 , 15 February 2023 | Updated : 21:31 , 15 February 2023 1.4k View comments Shield your eyes , Greta ! A 17-year-old schoolboy has exposed the gargantuan carbon footprint of some of America 's most flagrant gas guzzlers by tallying up the Co2 emissions from all their private jet flights in 2022 . At the top of the list is Silicon Valley entrepreneur Tom Siebel - whose 2022 carbon footprint was nearly 300 times the size of the average American 's . Other offenders are Rupert Murdoch and his jet-setting family , Kim Kardashian , Kylie Jenner , Elon Musk , Mark Zuckerberg , Pitbull and Jeff Bezos . Repeat offender : Kim Kardashian owns her own plane and makes frequent use of it . She was 52nd on the list , taking 99 flights , emitting 1,486.24 metric tons of Co2 Calling the campaign the Climate Jets project ranks , Shendure , a self-professed physics whiz , told The New York Times Tuesday that he came up with the idea after Twitter decided to block an account that had been tracking the private flights of Elon Musk , in December . Musk , the site 's new owner , would also engage in mass suspensions of a half dozen journalists he claimed 'doxxed ' him for posting Tweets related to the tracked flights . Here to shame your carbon footprint : Akash Shendure , 17 At the time , the decision stoked fears over Musk 's position of power as the new head of one of the world 's premiere social media platforms - and spurred Shendure to pursue his own tracking effort - this one geared toward affluent people in general who regularly fly in private jets . Looking to see what data was publicly available , the teenager quickly drew on what few resources were publicly available , gleaning the locations from various high-profile aircraft from a compilation created by a network of volunteers , and learning the carbon dioxide emissions for each type of fuel through government data . Identification numbers for the jets - many of which belong to businessmen and stars who have criticized efforts to quell global warming - came from a database designed by the 20-year-old operator of the since-pulled Elon tracker , Jack Sweeney . In 2021 , Musk offered Sweeney , 20 , $ 5,000 to take down the account , which he saw as a risk to his safety . In December , after taking the reigns of the multibillion-dollar company , Musk pulled the account , arguing that profiles that publish where people were located in 'real-time ' were a 'physical safety violation . ' Musk , meanwhile , also made Shendure 's list as the world 's 41st worst offender in regards to emissions , blamed for 1,699 tons this past year alone . Shendure 's calculations further found the Tesla CEO's private aircraft emissions surpass the collective carbon footprint of 109 average Americans , achieved after 190 flights in a single aircraft - or 14-and-half straight days in the air . The rapper Pitbull was 11th on the list , with a Co2 emission of 3,156.07 metric tons Now fully up in running , the interactive tracker , which constantly being updated in the real time , has unmasked some the most egregious culprits when it comes to air travel Many to make the list have now been unmasked as hypocrites by the Seattle high schooler , after criticizing others ' efforts to quell climate change . Pictured are offenders 13 through 29 , ranked by the emissions they generate each year through private jet travel Billionaires such as Bill Gates , Jeff Bezos , Mark Cuban , and stars like Jerry Seinfeld and Jay Z were also among the top offenders . Pictured are the rest of the well-off polluters that rounded out the top 50 That said , 40 others - including billionaires such as Bill Gates , Jeff Bezos , Mark Cuban , and stars like Kylie Jenner , Jerry Seinfeld , and Jay Z - far surpassed the outspoken Twitter honcho in terms of pollutants . But at the outset of his tracking journey in late December , Shendure told the Times that he still did not have the resources necessary to correctly tally the emissions of the world 's wealthiest residents . Speaking to the paper in an interview , he revealed he initially encountered difficulty when it came to calculating exactly how much fuel each jet consumes . However , in a stroke of luck , a company that sold this type of data - which was not named in the Times piece - inexplicably gifted the youngster the information when he reached out and asked . Shendure - who also runs a website that provides various math activities and services for students and teachers - reportedly told the company that he needed the information for an educational project . No laughing matter ! Billionaire Bill Gates was 13th on the list with a 2022 carbon footprint that is equal to that of 197 average Americans Kim Kardashian proudly showing off her own jet - Kim Air - in an episode of The Kardashians 'They sent it to me , which was really nice of them , ' he told the Times on Tuesday . After obtaining that elusive data , Shendure proceeded to analyze the information using programming languages Python and R , and went on to build the fully interactive website completely from scratch . Within weeks , Shendure said that he discovered that well over 100 people who own private jets are emitting at the very least dozens - and in many cases , hundreds - of times what the average American does in a year , solely through flying . The realization , Shendure said , came as particularly pronounced due to the fact that Americans boast some of the highest carbon emissions per capita in the world . 'It was surprising to me that this wasn ’ t being talked about more , ' he told the paper , adding that he spends most of his free time on the pet project . Now fully up in running , the interactive tracker , which constantly being updated in the real time , has successfully unmasked some the most egregious culprits when it comes to air travel . The worst polluter : CEO of C3 Energy , Thomas Siebel Simultaneously , many to make the list , as previously mentioned , have been unmasked as hypocrites by the Seattle high schooler . Siebel , along with the prominent Murdoch and DeVos families , rounded out the top three - who collectively account for the same amount of yearly emissions as nearly 850 ordinary Americans . Siebel , the 70-year-old founder of Silicon Valley software company Siebel Systems and the founder and chairman of AI firm C3.ai , has repeatedly voiced concerns over climate change . Just last month , the tech mogul - who is worth more than $ 3.5billion - penned an open letter touting the importance of AI in the country 's fight to address the ongoing phenomenon . In a piece titled 'Without AI , we won ’ t meet ESG goals and address climate change , ' Siebel wrote : 'The world is in a precarious condition due to climate change . 'Not surprisingly , companies are facing immense pressure from investors and customers to improve their transparency and performance on ESG issues , and many are getting positive feedback for their success . 'But the current state of environmental , social , and governance ( ESG ) programs is not making an adequate difference for climate change fast enough . ' Those assertions came from the single worst offender when it comes to air travel pollutants , who , according to Shendure , spent a full 35 days in the air in 2022 , taking more than 458 flights one three different private planes . Similarly , the second worst polluter according to Shendure 's calculations , News Corp kingpin Murdoch and his clan , recently expressed a desire to move his media outlets in his native Australia toward a more carbon-neutral future . The change of heart from the 91-year-old mogul - whose media empire includes Fox News - came years of casting doubt on global warming and attacking politicians who favored corrective action as a solution . The third worst offender , 65-year-old DeVos , is no different - in 2017 she underwent a swift about face in her previously neutral stance toward climate change , which saw the Trump staffer keep quiet about its inherent dangers . Last year , according to Shendure , the DeVos family - who live in a sprawling , multimillion-dollar , 22,000-square-foot mansion overlooking to Michigan's Lake Macatawa - took 597 private flights in three aircraft last year , for a total of 911.38 hours , or 37.97 days , in the air . In fourth place was American billionaire businessman and climate change advocate Samuel Zell , the founder and chairman of private investment firm Equity Group Investments . In 2018 , Zell ’ s company issued $ 400 million in green bonds , expected to generate proceeds of $ 396.7 million , which the Zell , 81 , at the time said would be funneled into green real estate projects . Last year , Zell took 381 flights in three separate aircraft , for a grand total of 795.81 hours in the air - or just over 33 days . Other billionaires such as Mike Bloomberg and Robert Kraft , both 81 , rounded out the top ten - with each man responsible for just under 3,200 tons of carbon dioxide emissions apiece . Elon Musk and Taylor Swift also appeared on the list of the 100 worst offenders Also near the top of the rankings were fierce climate change activists Gates , 67 , and rival Jeff Bezos - both of whom recently defended their private-plane use by saying the fuel they use is 'sustainable ' and thus eco-friendly . Gates , the 13th worst in terms of jet pollution , told BBC earlier this month after being pressed on whether or not he was a hypocrite for espousing climate change warnings - while jetting around the world on his massively polluting private plane - insisted that he was not part of the problem . Gates , who is worth $ 117billion , argued that because of the amount of money he pays Climeworks - a company which , for a monthly fee , offsets carbon emissions , he should be exempt from being grouped in with other tone-deaf jet setters . The Microsoft founder said not only was he 'comfortable ' with his position , but touted his sustainable-energy research company Breakthrough Energy . Bezos , meanwhile , came in 38th , with the Amazon boss ' jet travel the equivalent of that of 114 average Americans , according to the schoolboy 's calculations . Ahead of Bezos were several other A-listers including Seinfeld , Jay Z , and Mark Zuckerberg , all of whom have expressed a desire to address rising green house emissions . Nearing the top 50 were Kardashian , 42 , and storied director Stephen Spielberg , 76 , whose personal flights accounted for just under 1,500 metric tons of emissions each -equal to that of more than 99 Americans . Other notable names to grace the lists included Taylor Swift , 25-year-old Jenner , and Dr Phil , whose 2022 emissions each stood at more than 60 times that of the average American . Share what you think The comments below have not been moderated . The views expressed in the contents above are those of our users and do not necessarily reflect the views of MailOnline . By posting your comment you agree to our house rules . Do you want to automatically post your MailOnline comments to your Facebook Timeline ? Your comment will be posted to MailOnline as usual . Do you want to automatically post your MailOnline comments to your Facebook Timeline ? Your comment will be posted to MailOnline as usual We will automatically post your comment and a link to the news story to your Facebook timeline at the same time it is posted on MailOnline . To do this we will link your MailOnline account with your Facebook account . We ’ ll ask you to confirm this for your first post to Facebook . You can choose on each post whether you would like it to be posted to Facebook . Your details from Facebook will be used to provide you with tailored content , marketing and ads in line with our Privacy Policy . Published by Associated Newspapers Ltd Part of the Daily Mail , The Mail on Sunday & Metro Media Group\"}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_dataset = processed_dataset.map(lambda entry: {\"text\": \" \".join(entry[\"text\"])})\n",
    "joined_dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b5307e4-ba33-4483-b396-64da49a800d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "64f31d1c-4a03-497d-955b-0cffa75d0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fbb35d3e-97ea-467f-87ca-da0c6d8700b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f439f3651664f278627ddf7ef1d7a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55475877b65497bab3be1930aced4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = joined_dataset.map(\n",
    "    preprocess_data,\n",
    "    remove_columns=\"text\",\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c1231d-e2ba-4731-9256-be6888f980c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Verify dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "01929557-8708-49d0-8377-8a3dcce01e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "example = tokenized_dataset[\"train\"][3]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "31f53959-845d-4073-8f1d-5aad8ac222eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] By Associated Press Published : 18 : 40, 19 August 2019 | Updated : 19 : 00, 19 August 2019 PITTSBURGH ( AP ) - When celebrity chef Lidia Bastianich decided to open a restaurant in Pittsburgh's Strip District in 2001, she arrived in a neighborhood filled with warehouses and factories. This narrow stretch of streets in the shadow of the city's downtown office towers had long been home to food purveyors like Wholey's Fish Market and the Pennsylvania Macaroni Company, known to locals simply as Penn Mac. But a high - end restaurant helmed by a James Beard award - winning chef? That wasn't something anyone expected. Nearly two decades later, as Bastianich's eponymous Pittsburgh restaurant is set to close in September, the neighborhood around it has changed dramatically. Along what is now called Robotics Row, tech startups vie for office space in new buildings while Argo AI tests autonomous cars. In the process, Pittsburgh's restaurant scene has become almost as unrecognizable. The city has always had spots where you could drop in for a memorable meal, from the upscale Monterey Bay Fish Grotto on Mount Washington to the always - satisfying French fries at Essie's Original Hot Dog Shop in Oakland. This undated photo provided by the Galley Group shows the exterior of Smallman Galley in Pittsburgh's Strip District, once an industrial neighborhood and now home to robotics startups and Argo AI. A mix of homegrown chefs and transplants from other cities have been opening risk - taking and award - winning restaurants in Pittsburgh, offering creative spins on American food and authentic takes on cuisines like Venezuelan and Vietnamese that would have been hard to find less than a decade ago. ( Galley Group via AP ) But a mix of homegrown chefs and transplants from other cities have been opening risk - taking and award - winning restaurants, offering creative spins on American food and authentic takes on cuisines like Venezuelan and Vietnamese that would have been hard to find less than a decade ago. Earlier this year, the BBC called Pittsburgh ` ` the one destination foodies shouldn't miss in 2019,'' and The Washington Post noted that Pittsburgh has ` ` cuisine worth writing home about.'' Ben Mantica, who co - founded Pittsburgh's popular food hall Smallman Galley in 2015 and followed up with Federal Galley in 2017, credits the restaurant revival [SEP]\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9820a5-b315-4fc2-b01b-8b57ce1f377a",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "595efe69-78aa-4cd4-938c-8d65c88e0bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# use_fast uses fast tokenizers backed by rust. Remove it if it causes errors\n",
    "model = AutoModelForNextSentencePrediction.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e970785-d836-4ffb-941e-d9c271ef6ce2",
   "metadata": {},
   "source": [
    "### Verify data-model interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "999979a2-ee24-45e1-9433-7b9b001c9b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "# outputs = model(\n",
    "# input_ids=tokenized_dataset[labels[0]][\"train\"][\"input_ids\"][0],\n",
    "# labels=tokenized_dataset[labels[0]][\"train\"][0][\"labels\"],\n",
    "# )\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24902ec3-5c93-48d6-a4ec-980cd327eee2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "059ff917-8618-4bc2-819f-10da7b2849dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"accuracy\": evaluate.load(\"accuracy\"),\n",
    "    \"presicion\": evaluate.load(\"precision\"),\n",
    "    \"recall\": evaluate.load(\"recall\"),\n",
    "    \"f1\": evaluate.load(\"f1\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2a724497-7a15-48f8-ba60-95f7d5f0c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        name: metric.compute(predictions=predictions, references=labels)\n",
    "        for name, metric in metrics.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716eedb7-f61e-471a-bd17-9750bfc07bdc",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3fc962c2-e601-4f0f-9261-4d2cd1af5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8  # TODO: increase if we have more data\n",
    "num_epochs = 30\n",
    "# metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a47ca33e-84bc-4672-bd83-262d4d6f7abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    f\"bert_ai\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"all\",\n",
    "    # load_best_model_at_end=True,\n",
    "    # metric_for_best_model=metric_name,\n",
    "    # push_to_hub=True,  # TODO: enable once model seems good\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "259e2f4b-051a-4de4-a09b-c71a7cdcc15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fa6bc9b3-2cde-4db2-b6d3-9bc871cc9802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xt0r3/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6627\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 24870\n",
      "  Number of trainable parameters = 108311810\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,token_type_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [88], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1540\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1541\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1542\u001b[0m )\n\u001b[0;32m-> 1543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:1791\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1789\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1791\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1794\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1796\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1797\u001b[0m ):\n\u001b[1;32m   1798\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1799\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:2539\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2538\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2539\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2542\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:2584\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[0;32m-> 2584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(outputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   2589\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,token_type_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b028df0-8be7-409e-a3ce-1c676eb1e9fa",
   "metadata": {},
   "source": [
    "## Upload the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315b1085-f7d7-4a3f-a761-67053f8cc87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agency-vs-rest/checkpoint-263: 0.75 precision, 0.85 recall\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003bb89c-8ee0-4bc6-9316-cce5ebd5ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
