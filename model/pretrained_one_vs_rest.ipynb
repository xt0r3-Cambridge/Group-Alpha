{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79688b38-ed00-4f35-b491-6a41e2433a6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pretrained One-vs-Rest Classifier\n",
    "\n",
    "This notebook implements an one-vs-rest classifier that fine-tunes several BERT models to tell if a sentence contains problematic metaphors. It starts out from our BERT model pretrained on a large corpus of data.\n",
    "\n",
    "<div hidden>\n",
    "TODO: add extend data3/data.json with better data in the same format that actually makes sense.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c329b4-dc31-4cd0-8058-b59015328184",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f0b50b-c9ff-45b5-979a-c2fa5634ff27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers -Uqq\n",
    "%pip install sklearn -Uqq\n",
    "%pip install datasets -Uqq\n",
    "%pip install torch -Uqq\n",
    "%pip install numpy -Uqq\n",
    "%pip install evaluate -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d01c2eb7-f752-4628-b88d-48576c6461ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import os\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ae221-1bbc-4a4a-8a1e-0feb9d17b57e",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "898408be-0c15-4e58-9e7e-97fffccb4797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7240e6a4daa4bdb98137c591818529f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'agency', 'humanComparison', 'hyperbole', 'historyComparison', 'unjustClaims', 'deepSounding', 'sceptics', 'deEmphasize', 'performanceNumber', 'inscrutable'],\n",
       "        num_rows: 1022\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"data/data.json\", field=\"data\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b214393b-11e3-454e-b21f-a05ee38fefbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['A new vision of artificial intelligence for the people',\n",
       "  'The gig workers fighting back against the algorithms',\n",
       "  'How the AI industry profits from catastrophe'],\n",
       " 'agency': [False, True, False],\n",
       " 'humanComparison': [False, True, False],\n",
       " 'hyperbole': [False, True, True],\n",
       " 'historyComparison': [False, False, False],\n",
       " 'unjustClaims': [False, False, False],\n",
       " 'deepSounding': [False, False, False],\n",
       " 'sceptics': [False, False, False],\n",
       " 'deEmphasize': [False, False, False],\n",
       " 'performanceNumber': [False, False, False],\n",
       " 'inscrutable': [False, False, False]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce458fc2-c419-4b3c-b4aa-14571f7467e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agency',\n",
       " 'humanComparison',\n",
       " 'hyperbole',\n",
       " 'historyComparison',\n",
       " 'unjustClaims',\n",
       " 'deepSounding',\n",
       " 'sceptics',\n",
       " 'deEmphasize',\n",
       " 'performanceNumber',\n",
       " 'inscrutable']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for label in dataset[\"train\"].features.keys() if label not in [\"text\"]]\n",
    "\n",
    "num_epochs = {\n",
    "    \"agency\": 10,\n",
    "    \"humanComparison\": 2,\n",
    "    \"hyperbole\": 2,\n",
    "    \"uncriticalHistoryComparison\": 2,\n",
    "    \"unjustifiedClaimsAboutFuture\": 5,\n",
    "    \"deepSounding\": 2,\n",
    "    \"sceptics\": 2,\n",
    "    \"deEmphasize\": 7,\n",
    "    \"performanceNumber\": 2,\n",
    "    \"inscrutable\": 2,\n",
    "}\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514633be-491c-4f3c-a884-8fa657c17ce1",
   "metadata": {},
   "source": [
    "## Preprocess Data, Create Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9348e944-7ebd-4d28-ae74-a6b2c2f116fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-faae32b27e25bfec.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-aa385659ca8667d4.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-848a2cf3fb2b0f4d.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-9d22fa26c632e3c9.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-9dbc4328bbbbd751.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-1be1e4f62d4130bb.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2904c89ecf8e9791.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-83c855d38782a847.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-0b0914e3dbad8459.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-0344ca150e350639.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-f8bac47af80ce30f.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-cbb07536307f4581.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-90bb1cf88a4583c3.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-04678081a3f46aab.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-b878f506790cad81.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-9edf27e38105a798.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-54f7863ccf434074.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-bb57e9b498a1a754.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-fa6c8cb1e7433f70.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-6af032109d6634df.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2da101fca5221034.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-37e86a2d6adb5566.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-b3b5ebefbf7f5eba.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-6381449f56df583a.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-f28c1c034a36d0c4.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-47e8512965bb5a93.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-1fdd70ffa4d2742e.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-389c1213b4a56f6f.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-9368e12e08ef2db7.arrow\n",
      "Loading cached processed dataset at /home/xt0r3/.cache/huggingface/datasets/json/default-08d0520c01c5ad03/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-598c2aef70165e7b.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'agency': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 817\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 205\n",
       "     })\n",
       " }),\n",
       " 'humanComparison': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 817\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 205\n",
       "     })\n",
       " }),\n",
       " 'hyperbole': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 817\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 205\n",
       "     })\n",
       " }),\n",
       " 'historyComparison': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 817\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 205\n",
       "     })\n",
       " }),\n",
       " 'unjustClaims': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 817\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 205\n",
       "     })\n",
       " }),\n",
       " 'deepSounding': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 817\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 205\n",
       "     })\n",
       " }),\n",
       " 'sceptics': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 817\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 205\n",
       "     })\n",
       " }),\n",
       " 'deEmphasize': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 817\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 205\n",
       "     })\n",
       " }),\n",
       " 'performanceNumber': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 817\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 205\n",
       "     })\n",
       " }),\n",
       " 'inscrutable': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 817\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 205\n",
       "     })\n",
       " })}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset = {}\n",
    "for label in labels:\n",
    "    projected_dataset = (\n",
    "        dataset[\"train\"]\n",
    "        .map(remove_columns=[l for l in labels if l != label])\n",
    "        .rename_column(label, \"labels\")\n",
    "        .class_encode_column(\"labels\")\n",
    "    )\n",
    "    processed_dataset[label] = projected_dataset.train_test_split(\n",
    "        test_size=0.2, stratify_by_column=\"labels\"\n",
    "    )\n",
    "    # print(f\"{label}:\\n\\t{processed_dataset[label]['test'][0:3]}\\n\")\n",
    "\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f31d1c-4a03-497d-955b-0cffa75d0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbb35d3e-97ea-467f-87ca-da0c6d8700b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = {\n",
    "    k: ds.map(\n",
    "        preprocess_data,\n",
    "        remove_columns=\"text\",\n",
    "        batched=True,\n",
    "    )\n",
    "    for k, ds in processed_dataset.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c1231d-e2ba-4731-9256-be6888f980c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Verify dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01929557-8708-49d0-8377-8a3dcce01e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "example = tokenized_dataset[\"agency\"][\"train\"][0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31f53959-845d-4073-8f1d-5aad8ac222eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] NASA invites you to explore Mars though images of the Red Planet taken by its Curiosity rover and help train its AI algorithm by labelling rocks and surface features as you go [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8092ca8e-46fd-43ad-a7be-88e94055b695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9820a5-b315-4fc2-b01b-8b57ce1f377a",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "595efe69-78aa-4cd4-938c-8d65c88e0bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_fast uses fast tokenizers backed by rust. Remove it if it causes errors\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    # \"bert-base-cased\",\n",
    "    # num_labels=2,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e970785-d836-4ffb-941e-d9c271ef6ce2",
   "metadata": {},
   "source": [
    "### Verify data-model interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "999979a2-ee24-45e1-9433-7b9b001c9b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "# outputs = model(\n",
    "# input_ids=tokenized_dataset[labels[0]][\"train\"][\"input_ids\"][0],\n",
    "# labels=tokenized_dataset[labels[0]][\"train\"][0][\"labels\"],\n",
    "# )\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24902ec3-5c93-48d6-a4ec-980cd327eee2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "059ff917-8618-4bc2-819f-10da7b2849dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"accuracy\": evaluate.load(\"accuracy\"),\n",
    "    \"presicion\": evaluate.load(\"precision\"),\n",
    "    \"recall\": evaluate.load(\"recall\"),\n",
    "    \"f1\": evaluate.load(\"f1\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a724497-7a15-48f8-ba60-95f7d5f0c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    values = {}\n",
    "    \n",
    "    for name, metric in metrics.items():\n",
    "        result = metric.compute(predictions=predictions, references=labels)\n",
    "        for val in result.values() if isinstance(result, dict) else [result]:\n",
    "            values[name] = val\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3493053e-1b83-458c-bbd6-6bbe1140705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighPrecisionTrainer(Trainer):\n",
    "    \"\"\"A trainer class, which computes loss based on a weighted MSE, where the error for the positive labels is\n",
    "    weighted more than the error for the negative labels, leading to a higher precision\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        label_mask = torch.FloatTensor(1, 2).cuda().zero_()\n",
    "        label_mask[0, labels[0]] = 1\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss = torch.sum(torch.tensor([[1, 300]]).cuda() * ((logits - label_mask) ** 2))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716eedb7-f61e-471a-bd17-9750bfc07bdc",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fc962c2-e601-4f0f-9261-4d2cd1af5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # TODO: increase if we have more data\n",
    "metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa6bc9b3-2cde-4db2-b6d3-9bc871cc9802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 1\n",
      "model: xt0r3/aihype_article_bert_fine_tune\n",
      "weight decay: 0.0\n",
      "training model for agency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xt0r3/aihype_article_bert_fine_tune were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at xt0r3/aihype_article_bert_fine_tune and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/xt0r3/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 817\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8170\n",
      "  Number of trainable parameters = 108311810\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8170' max='8170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8170/8170 15:38, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Presicion</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.112100</td>\n",
       "      <td>1.012461</td>\n",
       "      <td>0.751220</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.556522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.753100</td>\n",
       "      <td>0.987382</td>\n",
       "      <td>0.795122</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.374900</td>\n",
       "      <td>1.356253</td>\n",
       "      <td>0.809756</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.589474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>1.563559</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.660377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.042100</td>\n",
       "      <td>1.959850</td>\n",
       "      <td>0.775610</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.634921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>1.613110</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.632653</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>1.710962</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>1.772866</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.784649</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>1.792016</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-817\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-817/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-817/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-1634\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-1634/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-1634/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-2451\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-2451/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-2451/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-817] due to args.save_total_limit\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-1634] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-3268\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-3268/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-3268/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-2451] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-4085\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-4085/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-4085/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-4902\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-4902/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-4902/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-4085] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-5719\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-5719/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-5719/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-4902] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-6536\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-6536/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-6536/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-5719] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-7353\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-7353/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-7353/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-6536] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-8170\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-8170/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-8170/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-7353] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-3268 (score: 0.660377358490566).\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_0-vs-rest/checkpoint-8170] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 2\n",
      "model: xt0r3/aihype_article_bert_fine_tune\n",
      "weight decay: 0.0\n",
      "training model for agency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/xt0r3/.cache/huggingface/hub/models--xt0r3--aihype_article_bert_fine_tune/snapshots/f77d86d05db534f13f5369d55df707e549baea8b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"xt0r3/aihype_article_bert_fine_tune\",\n",
      "  \"architectures\": [\n",
      "    \"BertForNextSentencePrediction\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/xt0r3/.cache/huggingface/hub/models--xt0r3--aihype_article_bert_fine_tune/snapshots/f77d86d05db534f13f5369d55df707e549baea8b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xt0r3/aihype_article_bert_fine_tune were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at xt0r3/aihype_article_bert_fine_tune and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "/home/xt0r3/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 817\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4090\n",
      "  Number of trainable parameters = 108311810\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4090' max='4090' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4090/4090 11:34, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Presicion</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.756246</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.776500</td>\n",
       "      <td>0.682435</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.690909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.538400</td>\n",
       "      <td>0.906221</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.217900</td>\n",
       "      <td>1.356719</td>\n",
       "      <td>0.790244</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.441558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>1.193272</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.638298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>1.179147</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.686869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>1.342709</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.590909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>1.260460</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>1.297034</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.647059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>1.404922</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.606742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-409\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-409/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-409/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-818\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-818/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-818/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-409] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-1227\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-1227/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-1227/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-1636\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-1636/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-1636/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-1227] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-2045\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-2045/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-2045/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-1636] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-2454\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-2454/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-2454/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-2045] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-2863\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-2863/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-2863/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-2454] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-3272\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-3272/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-3272/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-2863] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-3681\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-3681/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-3681/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-3272] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-4090\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-4090/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-4090/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-3681] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-818 (score: 0.6909090909090908).\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_1-vs-rest/checkpoint-4090] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 4\n",
      "model: xt0r3/aihype_article_bert_fine_tune\n",
      "weight decay: 0.0\n",
      "training model for agency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/xt0r3/.cache/huggingface/hub/models--xt0r3--aihype_article_bert_fine_tune/snapshots/f77d86d05db534f13f5369d55df707e549baea8b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"xt0r3/aihype_article_bert_fine_tune\",\n",
      "  \"architectures\": [\n",
      "    \"BertForNextSentencePrediction\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/xt0r3/.cache/huggingface/hub/models--xt0r3--aihype_article_bert_fine_tune/snapshots/f77d86d05db534f13f5369d55df707e549baea8b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xt0r3/aihype_article_bert_fine_tune were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at xt0r3/aihype_article_bert_fine_tune and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "/home/xt0r3/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 817\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2050\n",
      "  Number of trainable parameters = 108311810\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2050' max='2050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2050/2050 09:21, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Presicion</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.685358</td>\n",
       "      <td>0.760976</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.075472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.577540</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.693069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.430300</td>\n",
       "      <td>0.859286</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.567901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.430300</td>\n",
       "      <td>1.027272</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.647059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>1.179975</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.614035</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.648148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>1.272975</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.643478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>1.235009</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.610526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>1.279324</td>\n",
       "      <td>0.809756</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.597938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>1.283253</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.610526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>1.297175</td>\n",
       "      <td>0.809756</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.597938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-205\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-205/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-205/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-410\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-410/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-410/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-205] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-615\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-615/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-615/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-820\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-820/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-820/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-615] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-1025\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-1025/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-1025/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-820] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-1230\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-1230/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-1230/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-1025] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-1435\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-1435/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-1435/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-1230] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-1640\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-1640/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-1640/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-1435] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-1845\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-1845/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-1845/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-1640] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-2050\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-2050/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-2050/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-1845] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-410 (score: 0.693069306930693).\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_2-vs-rest/checkpoint-2050] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 8\n",
      "model: xt0r3/aihype_article_bert_fine_tune\n",
      "weight decay: 0.0\n",
      "training model for agency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/xt0r3/.cache/huggingface/hub/models--xt0r3--aihype_article_bert_fine_tune/snapshots/f77d86d05db534f13f5369d55df707e549baea8b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"xt0r3/aihype_article_bert_fine_tune\",\n",
      "  \"architectures\": [\n",
      "    \"BertForNextSentencePrediction\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/xt0r3/.cache/huggingface/hub/models--xt0r3--aihype_article_bert_fine_tune/snapshots/f77d86d05db534f13f5369d55df707e549baea8b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xt0r3/aihype_article_bert_fine_tune were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at xt0r3/aihype_article_bert_fine_tune and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "/home/xt0r3/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 817\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1030\n",
      "  Number of trainable parameters = 108311810\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1030' max='1030' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1030/1030 09:44, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Presicion</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.449628</td>\n",
       "      <td>0.751220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.382248</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.654206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.623104</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.574713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.887031</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.248500</td>\n",
       "      <td>1.170117</td>\n",
       "      <td>0.809756</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.248500</td>\n",
       "      <td>1.172929</td>\n",
       "      <td>0.795122</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.248500</td>\n",
       "      <td>1.164809</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.568421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.248500</td>\n",
       "      <td>1.216754</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.248500</td>\n",
       "      <td>1.262537</td>\n",
       "      <td>0.795122</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.553191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>1.263465</td>\n",
       "      <td>0.795122</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 8\n",
      "/home/xt0r3/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-103\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-103/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-103/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-206\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-206/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-206/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-103] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-309\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-309/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-309/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-412\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-412/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-412/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-309] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-515\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-515/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-515/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-206] due to args.save_total_limit\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-412] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-618\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-618/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-618/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-721\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-721/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-721/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-618] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-824\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-824/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-824/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-721] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-927\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-927/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-927/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-824] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-1030\n",
      "Configuration saved in xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-1030/config.json\n",
      "Model weights saved in xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-1030/pytorch_model.bin\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-927] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-515 (score: 0.6666666666666667).\n",
      "Deleting older checkpoint [xt0r3_aihype_article_bert_fine_tune_agency_3-vs-rest/checkpoint-1030] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 1\n",
      "model: bert-base-cased\n",
      "weight decay: 0.0\n",
      "training model for agency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/xt0r3/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/xt0r3/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "/home/xt0r3/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 817\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8170\n",
      "  Number of trainable parameters = 108311810\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8170' max='8170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8170/8170 18:48, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Presicion</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.106100</td>\n",
       "      <td>0.895397</td>\n",
       "      <td>0.790244</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.469136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.798500</td>\n",
       "      <td>1.025748</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.472222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.562100</td>\n",
       "      <td>1.136107</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.627907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.205300</td>\n",
       "      <td>1.729191</td>\n",
       "      <td>0.790244</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.612613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>1.485779</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>1.547218</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>1.826707</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.617021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>1.800239</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.617021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.793053</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>1.804079</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-base-cased_agency_0-vs-rest/checkpoint-817\n",
      "Configuration saved in bert-base-cased_agency_0-vs-rest/checkpoint-817/config.json\n",
      "Model weights saved in bert-base-cased_agency_0-vs-rest/checkpoint-817/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-base-cased_agency_0-vs-rest/checkpoint-1634\n",
      "Configuration saved in bert-base-cased_agency_0-vs-rest/checkpoint-1634/config.json\n",
      "Model weights saved in bert-base-cased_agency_0-vs-rest/checkpoint-1634/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency_0-vs-rest/checkpoint-817] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-base-cased_agency_0-vs-rest/checkpoint-2451\n",
      "Configuration saved in bert-base-cased_agency_0-vs-rest/checkpoint-2451/config.json\n",
      "Model weights saved in bert-base-cased_agency_0-vs-rest/checkpoint-2451/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency_0-vs-rest/checkpoint-1634] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-base-cased_agency_0-vs-rest/checkpoint-3268\n",
      "Configuration saved in bert-base-cased_agency_0-vs-rest/checkpoint-3268/config.json\n",
      "Model weights saved in bert-base-cased_agency_0-vs-rest/checkpoint-3268/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-base-cased_agency_0-vs-rest/checkpoint-4085\n",
      "Configuration saved in bert-base-cased_agency_0-vs-rest/checkpoint-4085/config.json\n",
      "Model weights saved in bert-base-cased_agency_0-vs-rest/checkpoint-4085/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency_0-vs-rest/checkpoint-3268] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-base-cased_agency_0-vs-rest/checkpoint-4902\n",
      "Configuration saved in bert-base-cased_agency_0-vs-rest/checkpoint-4902/config.json\n",
      "Model weights saved in bert-base-cased_agency_0-vs-rest/checkpoint-4902/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency_0-vs-rest/checkpoint-4085] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-base-cased_agency_0-vs-rest/checkpoint-5719\n",
      "Configuration saved in bert-base-cased_agency_0-vs-rest/checkpoint-5719/config.json\n",
      "Model weights saved in bert-base-cased_agency_0-vs-rest/checkpoint-5719/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency_0-vs-rest/checkpoint-4902] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-base-cased_agency_0-vs-rest/checkpoint-6536\n",
      "Configuration saved in bert-base-cased_agency_0-vs-rest/checkpoint-6536/config.json\n",
      "Model weights saved in bert-base-cased_agency_0-vs-rest/checkpoint-6536/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency_0-vs-rest/checkpoint-5719] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-base-cased_agency_0-vs-rest/checkpoint-7353\n",
      "Configuration saved in bert-base-cased_agency_0-vs-rest/checkpoint-7353/config.json\n",
      "Model weights saved in bert-base-cased_agency_0-vs-rest/checkpoint-7353/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency_0-vs-rest/checkpoint-6536] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-base-cased_agency_0-vs-rest/checkpoint-8170\n",
      "Configuration saved in bert-base-cased_agency_0-vs-rest/checkpoint-8170/config.json\n",
      "Model weights saved in bert-base-cased_agency_0-vs-rest/checkpoint-8170/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency_0-vs-rest/checkpoint-7353] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from bert-base-cased_agency_0-vs-rest/checkpoint-2451 (score: 0.627906976744186).\n",
      "Deleting older checkpoint [bert-base-cased_agency_0-vs-rest/checkpoint-8170] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 2\n",
      "model: bert-base-cased\n",
      "weight decay: 0.0\n",
      "training model for agency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/xt0r3/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/xt0r3/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "/home/xt0r3/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 817\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4090\n",
      "  Number of trainable parameters = 108311810\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3445' max='4090' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3445/4090 09:59 < 01:52, 5.75 it/s, Epoch 8.42/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Presicion</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.699402</td>\n",
       "      <td>0.770732</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.145455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.741600</td>\n",
       "      <td>0.776101</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.554217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.592700</td>\n",
       "      <td>0.886389</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.630631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.357900</td>\n",
       "      <td>1.050515</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.574713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>1.444456</td>\n",
       "      <td>0.790244</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.565657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>1.757508</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.507246</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>1.547263</td>\n",
       "      <td>0.785366</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.531915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>1.594403</td>\n",
       "      <td>0.795122</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.553191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to bert-base-cased_agency_1-vs-rest/checkpoint-409\n",
      "Configuration saved in bert-base-cased_agency_1-vs-rest/checkpoint-409/config.json\n",
      "Model weights saved in bert-base-cased_agency_1-vs-rest/checkpoint-409/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to bert-base-cased_agency_1-vs-rest/checkpoint-818\n",
      "Configuration saved in bert-base-cased_agency_1-vs-rest/checkpoint-818/config.json\n",
      "Model weights saved in bert-base-cased_agency_1-vs-rest/checkpoint-818/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency_1-vs-rest/checkpoint-409] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to bert-base-cased_agency_1-vs-rest/checkpoint-1227\n",
      "Configuration saved in bert-base-cased_agency_1-vs-rest/checkpoint-1227/config.json\n",
      "Model weights saved in bert-base-cased_agency_1-vs-rest/checkpoint-1227/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency_1-vs-rest/checkpoint-818] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to bert-base-cased_agency_1-vs-rest/checkpoint-1636\n",
      "Configuration saved in bert-base-cased_agency_1-vs-rest/checkpoint-1636/config.json\n",
      "Model weights saved in bert-base-cased_agency_1-vs-rest/checkpoint-1636/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to bert-base-cased_agency_1-vs-rest/checkpoint-2045\n",
      "Configuration saved in bert-base-cased_agency_1-vs-rest/checkpoint-2045/config.json\n",
      "Model weights saved in bert-base-cased_agency_1-vs-rest/checkpoint-2045/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency_1-vs-rest/checkpoint-1636] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to bert-base-cased_agency_1-vs-rest/checkpoint-2454\n",
      "Configuration saved in bert-base-cased_agency_1-vs-rest/checkpoint-2454/config.json\n",
      "Model weights saved in bert-base-cased_agency_1-vs-rest/checkpoint-2454/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency_1-vs-rest/checkpoint-2045] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to bert-base-cased_agency_1-vs-rest/checkpoint-2863\n",
      "Configuration saved in bert-base-cased_agency_1-vs-rest/checkpoint-2863/config.json\n",
      "Model weights saved in bert-base-cased_agency_1-vs-rest/checkpoint-2863/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency_1-vs-rest/checkpoint-2454] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to bert-base-cased_agency_1-vs-rest/checkpoint-3272\n",
      "Configuration saved in bert-base-cased_agency_1-vs-rest/checkpoint-3272/config.json\n",
      "Model weights saved in bert-base-cased_agency_1-vs-rest/checkpoint-3272/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency_1-vs-rest/checkpoint-2863] due to args.save_total_limit\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, weight_decay in product(['xt0r3/aihype_article_bert_fine_tune', 'bert-base-cased'], [0.0]):  # , 0.01]):\n",
    "    for i in range(0, 4):\n",
    "        batch_size = 2 ** i\n",
    "        print(f'batch size: {2 ** i}\\nmodel: {model_name}\\nweight decay: {weight_decay}')\n",
    "\n",
    "        for label in ['agency']:  # labels:\n",
    "            print(f\"training model for {label}\")\n",
    "\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_name,\n",
    "                num_labels=2,\n",
    "            )\n",
    "\n",
    "            training_args = TrainingArguments(\n",
    "                f\"{model_name.replace('/', '_')}_{label}_{i}-vs-rest\",\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                save_strategy=\"epoch\",\n",
    "                learning_rate=2e-5,\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                per_device_eval_batch_size=batch_size,\n",
    "                num_train_epochs=num_epochs[label],\n",
    "                weight_decay=weight_decay,\n",
    "                report_to=\"none\",\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=metric_name,\n",
    "                save_total_limit = 1,\n",
    "                # push_to_hub=True,  # TODO: enable once model seems good\n",
    "            )\n",
    "\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_dataset[label][\"train\"],\n",
    "                eval_dataset=tokenized_dataset[label][\"test\"],\n",
    "                compute_metrics=compute_metrics,\n",
    "            )\n",
    "\n",
    "            trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b028df0-8be7-409e-a3ce-1c676eb1e9fa",
   "metadata": {},
   "source": [
    "## Upload the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315b1085-f7d7-4a3f-a761-67053f8cc87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agency-vs-rest/checkpoint-263: 0.75 precision, 0.85 recall\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003bb89c-8ee0-4bc6-9316-cce5ebd5ddb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
