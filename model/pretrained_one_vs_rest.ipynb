{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79688b38-ed00-4f35-b491-6a41e2433a6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pretrained One-vs-Rest Classifier\n",
    "\n",
    "This notebook implements an one-vs-rest classifier that fine-tunes several BERT models to tell if a sentence contains problematic metaphors. It starts out from our BERT model pretrained on a large corpus of data.\n",
    "\n",
    "<div hidden>\n",
    "TODO: add extend data3/data.json with better data in the same format that actually makes sense.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c329b4-dc31-4cd0-8058-b59015328184",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f0b50b-c9ff-45b5-979a-c2fa5634ff27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers -Uqq\n",
    "%pip install sklearn -Uqq\n",
    "%pip install datasets -Uqq\n",
    "%pip install torch -Uqq\n",
    "%pip install numpy -Uqq\n",
    "%pip install evaluate -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d01c2eb7-f752-4628-b88d-48576c6461ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import os\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ae221-1bbc-4a4a-8a1e-0feb9d17b57e",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "898408be-0c15-4e58-9e7e-97fffccb4797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9deaee5413fa4f37ab7c5d2d8a9c40d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'agency', 'humanComparison', 'hyperbole', 'historyComparison', 'unjustClaims', 'deepSounding', 'sceptics', 'deEmphasize', 'performanceNumber', 'inscrutable'],\n",
       "        num_rows: 329\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"data/data.json\", field=\"data\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b214393b-11e3-454e-b21f-a05ee38fefbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['A new vision of artificial intelligence for the people',\n",
       "  'The gig workers fighting back against the algorithms',\n",
       "  'How the AI industry profits from catastrophe'],\n",
       " 'agency': [False, True, False],\n",
       " 'humanComparison': [False, True, False],\n",
       " 'hyperbole': [False, True, True],\n",
       " 'historyComparison': [False, False, False],\n",
       " 'unjustClaims': [False, False, False],\n",
       " 'deepSounding': [False, False, False],\n",
       " 'sceptics': [False, False, False],\n",
       " 'deEmphasize': [False, False, False],\n",
       " 'performanceNumber': [False, False, False],\n",
       " 'inscrutable': [False, False, False]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce458fc2-c419-4b3c-b4aa-14571f7467e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agency',\n",
       " 'humanComparison',\n",
       " 'hyperbole',\n",
       " 'historyComparison',\n",
       " 'unjustClaims',\n",
       " 'deepSounding',\n",
       " 'sceptics',\n",
       " 'deEmphasize',\n",
       " 'performanceNumber',\n",
       " 'inscrutable']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for label in dataset[\"train\"].features.keys() if label not in [\"text\"]]\n",
    "\n",
    "num_epochs = {\n",
    "    \"agency\": 10,\n",
    "    \"humanComparison\": 2,\n",
    "    \"hyperbole\": 2,\n",
    "    \"historyComparison\": 2,\n",
    "    \"unjustClaims\": 5,\n",
    "    \"deepSounding\": 2,\n",
    "    \"sceptics\": 2,\n",
    "    \"deEmphasize\": 7,\n",
    "    \"performanceNumber\": 2,\n",
    "    \"inscrutable\": 2,\n",
    "}\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514633be-491c-4f3c-a884-8fa657c17ce1",
   "metadata": {},
   "source": [
    "## Preprocess Data, Create Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9348e944-7ebd-4d28-ae74-a6b2c2f116fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-604182da521c301e.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-8d3dee6ea1033dcd.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-839d38437908d34a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-fc7d1db672f398d9.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-36f8b4671a097526.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-d1c689a04345753f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-65bae1c09a23b093.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-5c86dd9d5875baa7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-48d0460544fede1d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2d07211d428086f9.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-7ad65b6c56e5ee86.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-542607c85873867a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-8868f5bd79da0d88.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-4ca18615d9cd5755.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2dc6d19d9fb1d6dd.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-cc89a32b4186c025.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-78f03212d5343081.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-d65c026f6fa0febc.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-b6be7eaf717676e0.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-234d325e7d83cd6a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-5bffac98a60ff6b1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-adf368cad6a269a1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-c05bbd55892b7f41.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-5593cf2e947798dd.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-a40fd09267419943.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-bd1d8a9cc3d13cee.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e964b53254ab25e0.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-3370822a25a1587a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-aa53c4d8ec1b3b2d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e7751a31f3f17887/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-f689d417a4b1cb5c.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'agency': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 263\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 66\n",
       "     })\n",
       " }),\n",
       " 'humanComparison': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 263\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 66\n",
       "     })\n",
       " }),\n",
       " 'hyperbole': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 263\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 66\n",
       "     })\n",
       " }),\n",
       " 'historyComparison': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 263\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 66\n",
       "     })\n",
       " }),\n",
       " 'unjustClaims': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 263\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 66\n",
       "     })\n",
       " }),\n",
       " 'deepSounding': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 263\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 66\n",
       "     })\n",
       " }),\n",
       " 'sceptics': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 263\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 66\n",
       "     })\n",
       " }),\n",
       " 'deEmphasize': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 263\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 66\n",
       "     })\n",
       " }),\n",
       " 'performanceNumber': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 263\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 66\n",
       "     })\n",
       " }),\n",
       " 'inscrutable': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 263\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 66\n",
       "     })\n",
       " })}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset = {}\n",
    "for label in labels:\n",
    "    projected_dataset = (\n",
    "        dataset[\"train\"]\n",
    "        .map(remove_columns=[l for l in labels if l != label])\n",
    "        .rename_column(label, \"labels\")\n",
    "        .class_encode_column(\"labels\")\n",
    "    )\n",
    "    processed_dataset[label] = projected_dataset.train_test_split(\n",
    "        test_size=0.2, stratify_by_column=\"labels\"\n",
    "    )\n",
    "    # print(f\"{label}:\\n\\t{processed_dataset[label]['test'][0:3]}\\n\")\n",
    "\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64f31d1c-4a03-497d-955b-0cffa75d0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbb35d3e-97ea-467f-87ca-da0c6d8700b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = {\n",
    "    k: ds.map(\n",
    "        preprocess_data,\n",
    "        remove_columns=\"text\",\n",
    "        batched=True,\n",
    "    )\n",
    "    for k, ds in processed_dataset.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c1231d-e2ba-4731-9256-be6888f980c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Verify dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01929557-8708-49d0-8377-8a3dcce01e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "example = tokenized_dataset[\"agency\"][\"train\"][0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31f53959-845d-4073-8f1d-5aad8ac222eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Big Tech â€™ s guide to talking about AI ethics [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8092ca8e-46fd-43ad-a7be-88e94055b695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9820a5-b315-4fc2-b01b-8b57ce1f377a",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "595efe69-78aa-4cd4-938c-8d65c88e0bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_fast uses fast tokenizers backed by rust. Remove it if it causes errors\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    # \"bert-base-cased\",\n",
    "    # num_labels=2,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e970785-d836-4ffb-941e-d9c271ef6ce2",
   "metadata": {},
   "source": [
    "### Verify data-model interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "999979a2-ee24-45e1-9433-7b9b001c9b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "# outputs = model(\n",
    "# input_ids=tokenized_dataset[labels[0]][\"train\"][\"input_ids\"][0],\n",
    "# labels=tokenized_dataset[labels[0]][\"train\"][0][\"labels\"],\n",
    "# )\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24902ec3-5c93-48d6-a4ec-980cd327eee2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "059ff917-8618-4bc2-819f-10da7b2849dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"accuracy\": evaluate.load(\"accuracy\"),\n",
    "    \"presicion\": evaluate.load(\"precision\"),\n",
    "    \"recall\": evaluate.load(\"recall\"),\n",
    "    \"f1\": evaluate.load(\"f1\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a724497-7a15-48f8-ba60-95f7d5f0c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    values = {}\n",
    "    \n",
    "    for name, metric in metrics.items():\n",
    "        result = metric.compute(predictions=predictions, references=labels)\n",
    "        for val in result.values() if isinstance(result, dict) else [result]:\n",
    "            values[name] = val\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3493053e-1b83-458c-bbd6-6bbe1140705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighPrecisionTrainer(Trainer):\n",
    "    \"\"\"A trainer class, which computes loss based on a weighted MSE, where the error for the positive labels is\n",
    "    weighted more than the error for the negative labels, leading to a higher precision\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        label_mask = torch.FloatTensor(1, 2).cuda().zero_()\n",
    "        label_mask[0, labels[0]] = 1\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss = torch.sum(torch.tensor([[1, 300]]).cuda() * ((logits - label_mask) ** 2))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716eedb7-f61e-471a-bd17-9750bfc07bdc",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fc962c2-e601-4f0f-9261-4d2cd1af5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # TODO: increase if we have more data\n",
    "metric_name = \"recall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6bc9b3-2cde-4db2-b6d3-9bc871cc9802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 1\n",
      "model: bert-base-cased\n",
      "weight decay: 0.0\n",
      "training model for agency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 263\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2630\n",
      "  Number of trainable parameters = 108311810\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2630' max='2630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2630/2630 03:54, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Presicion</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.882871</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.018600</td>\n",
       "      <td>1.060922</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.018600</td>\n",
       "      <td>1.516878</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.117647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.303700</td>\n",
       "      <td>1.769249</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.303700</td>\n",
       "      <td>1.712236</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>1.835033</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>1.889408</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.885001</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.897952</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.900281</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to bert-base-cased_agency-vs-rest/checkpoint-263\n",
      "Configuration saved in bert-base-cased_agency-vs-rest/checkpoint-263/config.json\n",
      "Model weights saved in bert-base-cased_agency-vs-rest/checkpoint-263/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-526] due to args.save_total_limit\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-789] due to args.save_total_limit\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-1052] due to args.save_total_limit\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-1315] due to args.save_total_limit\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-1578] due to args.save_total_limit\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-1841] due to args.save_total_limit\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-2104] due to args.save_total_limit\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-2367] due to args.save_total_limit\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-2630] due to args.save_total_limit\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-132] due to args.save_total_limit\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-264] due to args.save_total_limit\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-396] due to args.save_total_limit\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-528] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to bert-base-cased_agency-vs-rest/checkpoint-526\n",
      "Configuration saved in bert-base-cased_agency-vs-rest/checkpoint-526/config.json\n",
      "Model weights saved in bert-base-cased_agency-vs-rest/checkpoint-526/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-660] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-base-cased_agency-vs-rest/checkpoint-789\n",
      "Configuration saved in bert-base-cased_agency-vs-rest/checkpoint-789/config.json\n",
      "Model weights saved in bert-base-cased_agency-vs-rest/checkpoint-789/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-263] due to args.save_total_limit\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-526] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-base-cased_agency-vs-rest/checkpoint-1052\n",
      "Configuration saved in bert-base-cased_agency-vs-rest/checkpoint-1052/config.json\n",
      "Model weights saved in bert-base-cased_agency-vs-rest/checkpoint-1052/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-base-cased_agency-vs-rest/checkpoint-1315\n",
      "Configuration saved in bert-base-cased_agency-vs-rest/checkpoint-1315/config.json\n",
      "Model weights saved in bert-base-cased_agency-vs-rest/checkpoint-1315/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-789] due to args.save_total_limit\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-1052] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-base-cased_agency-vs-rest/checkpoint-1578\n",
      "Configuration saved in bert-base-cased_agency-vs-rest/checkpoint-1578/config.json\n",
      "Model weights saved in bert-base-cased_agency-vs-rest/checkpoint-1578/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-base-cased_agency-vs-rest/checkpoint-1841\n",
      "Configuration saved in bert-base-cased_agency-vs-rest/checkpoint-1841/config.json\n",
      "Model weights saved in bert-base-cased_agency-vs-rest/checkpoint-1841/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-1578] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-base-cased_agency-vs-rest/checkpoint-2104\n",
      "Configuration saved in bert-base-cased_agency-vs-rest/checkpoint-2104/config.json\n",
      "Model weights saved in bert-base-cased_agency-vs-rest/checkpoint-2104/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-1841] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-base-cased_agency-vs-rest/checkpoint-2367\n",
      "Configuration saved in bert-base-cased_agency-vs-rest/checkpoint-2367/config.json\n",
      "Model weights saved in bert-base-cased_agency-vs-rest/checkpoint-2367/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-2104] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to bert-base-cased_agency-vs-rest/checkpoint-2630\n",
      "Configuration saved in bert-base-cased_agency-vs-rest/checkpoint-2630/config.json\n",
      "Model weights saved in bert-base-cased_agency-vs-rest/checkpoint-2630/pytorch_model.bin\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-2367] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from bert-base-cased_agency-vs-rest/checkpoint-1315 (score: 0.2857142857142857).\n",
      "Deleting older checkpoint [bert-base-cased_agency-vs-rest/checkpoint-2630] due to args.save_total_limit\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 2\n",
      "model: bert-base-cased\n",
      "weight decay: 0.0\n",
      "training model for agency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 263\n",
      "  Num Epochs = 12\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1584\n",
      "  Number of trainable parameters = 108311810\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='1584' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  26/1584 00:02 < 03:11, 8.14 it/s, Epoch 0.19/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model_name, weight_decay in product(['bert-base-cased', 'xt0r3/aihype_bert_fine_tune'], [0.0, 0.01]):\n",
    "    for i in range(0, 5):\n",
    "        batch_size = 2 ** i\n",
    "        print(f'batch size: {2 ** i}\\nmodel: {model_name}\\nweight decay: {weight_decay}')\n",
    "\n",
    "        for label in ['agency']:  # labels:\n",
    "            print(f\"training model for {label}\")\n",
    "\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_name,\n",
    "                num_labels=2,\n",
    "            )\n",
    "\n",
    "            training_args = TrainingArguments(\n",
    "                f\"{model_name.replace('/', '_')}_{label}-vs-rest\",\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                save_strategy=\"epoch\",\n",
    "                learning_rate=2e-5,\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                per_device_eval_batch_size=batch_size,\n",
    "                num_train_epochs=num_epochs[label] + i * 2,\n",
    "                weight_decay=weight_decay,\n",
    "                report_to=\"none\",\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=metric_name,\n",
    "                save_total_limit = 1,\n",
    "                # push_to_hub=True,  # TODO: enable once model seems good\n",
    "            )\n",
    "\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_dataset[label][\"train\"],\n",
    "                eval_dataset=tokenized_dataset[label][\"test\"],\n",
    "                compute_metrics=compute_metrics,\n",
    "            )\n",
    "\n",
    "            trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b028df0-8be7-409e-a3ce-1c676eb1e9fa",
   "metadata": {},
   "source": [
    "## Upload the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315b1085-f7d7-4a3f-a761-67053f8cc87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agency-vs-rest/checkpoint-263: 0.75 precision, 0.85 recall\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003bb89c-8ee0-4bc6-9316-cce5ebd5ddb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
